{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "name": "16-advanced-anomaly-detection.ipynb"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders & Real-Time Streaming Anomaly Detection\n",
    "\n",
    "From the [Sisyphean Gridworks ML Playground](https://sgridworks.com/ml-playground/guides/16-advanced-anomaly-detection.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Clone the repository and install dependencies. Run this cell first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone https://github.com/SGridworks/Dynamic-Network-Model.git 2>/dev/null || echo 'Already cloned'\n",
    "%cd Dynamic-Network-Model\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm pyarrow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Autoencoder to VAE \u2014 Why Probabilistic?\n",
    "\n",
    "In Guide 08, the basic autoencoder compressed voltage features into a fixed latent vector and reconstructed them. Anomalies had high reconstruction error. This works, but the latent space is unstructured\u2014there is no guarantee that nearby latent points represent similar voltage patterns, and there is no way to quantify how unlikely a given reading is."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "from demo_data.load_demo_data import load_customer_interval_data, load_weather_data, load_outage_history\n",
    "\n",
    "# Load AMI data (same as Guide 08)\n",
    "ami = load_customer_interval_data()\n",
    "ami[\"timestamp\"] = pd.to_datetime(ami[\"timestamp\"])\n",
    "\n",
    "# Aggregate to hourly statistics per customer (same features as Guide 08)\n",
    "hourly = ami.groupby([\"customer_id\", ami[\"timestamp\"].dt.floor(\"h\")]).agg(\n",
    "    voltage_mean=(\"voltage_v\", \"mean\"),\n",
    "    voltage_std=(\"voltage_v\", \"std\"),\n",
    "    voltage_min=(\"voltage_v\", \"min\"),\n",
    "    voltage_max=(\"voltage_v\", \"max\"),\n",
    "    energy_kwh=(\"energy_kwh\", \"sum\"),\n",
    ").reset_index()\n",
    "hourly[\"voltage_range\"] = hourly[\"voltage_max\"] - hourly[\"voltage_min\"]\n",
    "hourly = hourly.fillna(0)\n",
    "\n",
    "feature_cols = [\"voltage_mean\", \"voltage_std\", \"voltage_range\",\n",
    "                \"voltage_min\", \"voltage_max\", \"energy_kwh\"]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(hourly[feature_cols])\n",
    "\n",
    "print(f\"Hourly feature rows: {len(hourly):,}\")\n",
    "print(f\"Features: {feature_cols}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the VAE Architecture\n",
    "\n",
    "The VAE has three key differences from the basic autoencoder in Guide 08:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class VoltageVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=3):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder: shared layers\n",
    "        self.encoder_shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Two separate heads: one for mu, one for log_var\n",
    "        self.fc_mu      = nn.Linear(16, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(16, latent_dim)\n",
    "\n",
    "        # Decoder: reconstruct from latent sample\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, input_dim),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Return mu and log_var for the latent distribution.\"\"\"\n",
    "        h = self.encoder_shared(x)\n",
    "        mu      = self.fc_mu(h)\n",
    "        log_var = self.fc_log_var(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"Sample z using the reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)       # sample epsilon ~ N(0, 1)\n",
    "        z   = mu + std * eps              # shift and scale\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, log_var\n",
    "\n",
    "# Define the VAE loss function\n",
    "def vae_loss(x, x_recon, mu, log_var):\n",
    "    \"\"\"\n",
    "    ELBO loss = Reconstruction loss + KL divergence.\n",
    "    Returns total loss and both components separately for monitoring.\n",
    "    \"\"\"\n",
    "    # Reconstruction loss: mean squared error per sample\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction=\"mean\")\n",
    "\n",
    "    # KL divergence: D_KL(q(z|x) || p(z)) where p(z) = N(0, I)\n",
    "    # Closed-form for two Gaussians:\n",
    "    #   -0.5 * sum(1 + log_var - mu^2 - exp(log_var))\n",
    "    kl_loss = -0.5 * torch.mean(\n",
    "        torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1)\n",
    "    )\n",
    "\n",
    "    total_loss = recon_loss + kl_loss\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = len(feature_cols)\n",
    "vae = VoltageVAE(input_dim, latent_dim=3)\n",
    "print(vae)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in vae.parameters()):,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the VAE on Normal Voltage Data\n",
    "\n",
    "Following the same approach as Guide 08, we train exclusively on normal data. We first run an Isolation Forest to identify and exclude obvious anomalies from the training set, then train the VAE to learn the distribution of normal voltage patterns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pre-filter: remove obvious anomalies with Isolation Forest (same as Guide 08)\n",
    "iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42)\n",
    "iso_forest.fit(X_scaled)\n",
    "iso_labels = iso_forest.predict(X_scaled)\n",
    "\n",
    "# Keep only normal data for VAE training\n",
    "normal_mask = iso_labels == 1\n",
    "X_normal = X_scaled[normal_mask]\n",
    "print(f\"Training samples (normal only): {len(X_normal):,}\")\n",
    "\n",
    "# Train/validation split (80/20)\n",
    "split = int(len(X_normal) * 0.8)\n",
    "train_tensor = torch.FloatTensor(X_normal[:split])\n",
    "val_tensor   = torch.FloatTensor(X_normal[split:])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(train_tensor, train_tensor),\n",
    "    batch_size=128, shuffle=True\n",
    ")\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# Track losses separately\n",
    "history = {\"total\": [], \"recon\": [], \"kl\": []}\n",
    "\n",
    "# Train for 80 epochs\n",
    "for epoch in range(80):\n",
    "    vae.train()\n",
    "    epoch_total, epoch_recon, epoch_kl = 0, 0, 0\n",
    "\n",
    "    for batch_x, _ in train_loader:\n",
    "        x_recon, mu, log_var = vae(batch_x)\n",
    "        total, recon, kl = vae_loss(batch_x, x_recon, mu, log_var)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_total += total.item()\n",
    "        epoch_recon += recon.item()\n",
    "        epoch_kl    += kl.item()\n",
    "\n",
    "    n_batches = len(train_loader)\n",
    "    history[\"total\"].append(epoch_total / n_batches)\n",
    "    history[\"recon\"].append(epoch_recon / n_batches)\n",
    "    history[\"kl\"].append(epoch_kl / n_batches)\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:>3}/80  \"\n",
    "              f\"Total: {history['total'][-1]:.4f}  \"\n",
    "              f\"Recon: {history['recon'][-1]:.4f}  \"\n",
    "              f\"KL: {history['kl'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Probabilistic Anomaly Scores with the ELBO\n",
    "\n",
    "The basic autoencoder from Guide 08 uses reconstruction error as its anomaly score. The VAE gives us a richer signal: the Evidence Lower Bound (ELBO), which combines reconstruction probability with the KL divergence. A low ELBO means the data point is unlikely under the learned model\u2014it is both hard to reconstruct and its latent encoding is far from the prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot training curves: reconstruction loss and KL divergence separately\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history[\"recon\"], color=\"#5FCCDB\", label=\"Reconstruction Loss\")\n",
    "ax1.plot(history[\"kl\"], color=\"#D69E2E\", label=\"KL Divergence\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"VAE Training: Loss Components\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history[\"total\"], color=\"#2D6A7A\", linewidth=2)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Total ELBO Loss\")\n",
    "ax2.set_title(\"VAE Training: Total Loss (ELBO)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare VAE vs Autoencoder vs Isolation Forest\n",
    "\n",
    "To fairly compare all three approaches, we need ground-truth labels that are independent of any model. Using one model\u2019s output (e.g., Isolation Forest flags) as pseudo-ground-truth to evaluate that same model would be circular and methodologically invalid. Instead, we inject synthetic anomalies\u2014voltage sags, spikes, and drift\u2014into a held-out portion of the data at known timestamps. This gives us an objective, model-independent ground truth for precision-recall evaluation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Score ALL data points (normal + anomalous) through the VAE\n",
    "vae.eval()\n",
    "all_tensor = torch.FloatTensor(X_scaled)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_recon, mu, log_var = vae(all_tensor)\n",
    "\n",
    "    # Per-sample reconstruction error\n",
    "    recon_error = torch.mean((all_tensor - x_recon) ** 2, dim=1)\n",
    "\n",
    "    # Per-sample KL divergence\n",
    "    kl_per_sample = -0.5 * torch.sum(\n",
    "        1 + log_var - mu.pow(2) - log_var.exp(), dim=1\n",
    "    )\n",
    "\n",
    "    # Combined ELBO anomaly score (higher = more anomalous)\n",
    "    elbo_score = recon_error + kl_per_sample\n",
    "\n",
    "hourly[\"vae_recon_error\"] = recon_error.numpy()\n",
    "hourly[\"vae_kl_div\"]      = kl_per_sample.numpy()\n",
    "hourly[\"vae_elbo_score\"]  = elbo_score.numpy()\n",
    "\n",
    "# Set threshold at 99th percentile of ELBO score\n",
    "elbo_threshold = hourly[\"vae_elbo_score\"].quantile(0.99)\n",
    "hourly[\"vae_anomaly\"] = (hourly[\"vae_elbo_score\"] > elbo_threshold).astype(int)\n",
    "\n",
    "print(f\"ELBO threshold (99th pctile): {elbo_threshold:.4f}\")\n",
    "print(f\"VAE anomalies detected:       {hourly['vae_anomaly'].sum()}\")\n",
    "print(f\"\\nScore components (anomalies only):\")\n",
    "anomalous = hourly[hourly[\"vae_anomaly\"] == 1]\n",
    "print(f\"  Avg reconstruction error: {anomalous['vae_recon_error'].mean():.4f}\")\n",
    "print(f\"  Avg KL divergence:        {anomalous['vae_kl_div'].mean():.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design a Real-Time Detection Pipeline\n",
    "\n",
    "Everything so far has been batch detection: we load all the data, train a model, and score everything at once. In a real utility control room, AMI data arrives continuously\u2014every 15 minutes from thousands of customers. We need a system that processes data as it arrives, in sliding windows, and flags anomalies in near-real-time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize the two components of the anomaly score\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter: reconstruction error vs KL divergence\n",
    "normal_pts = hourly[hourly[\"vae_anomaly\"] == 0]\n",
    "anomaly_pts = hourly[hourly[\"vae_anomaly\"] == 1]\n",
    "\n",
    "ax1.scatter(normal_pts[\"vae_recon_error\"], normal_pts[\"vae_kl_div\"],\n",
    "            c=\"#5FCCDB\", s=3, alpha=0.2, label=\"Normal\")\n",
    "ax1.scatter(anomaly_pts[\"vae_recon_error\"], anomaly_pts[\"vae_kl_div\"],\n",
    "            c=\"red\", s=25, marker=\"x\", label=\"Anomaly\")\n",
    "ax1.set_xlabel(\"Reconstruction Error\")\n",
    "ax1.set_ylabel(\"KL Divergence\")\n",
    "ax1.set_title(\"VAE Anomaly Score Components\")\n",
    "ax1.legend()\n",
    "\n",
    "# ELBO score distribution\n",
    "ax2.hist(hourly[\"vae_elbo_score\"], bins=150, color=\"#5FCCDB\", edgecolor=\"white\")\n",
    "ax2.axvline(x=elbo_threshold, color=\"red\", linestyle=\"--\",\n",
    "           label=f\"Threshold ({elbo_threshold:.3f})\")\n",
    "ax2.set_xlabel(\"ELBO Anomaly Score\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "ax2.set_title(\"ELBO Score Distribution\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Sliding Window Processor\n",
    "\n",
    "The core of the streaming pipeline is a class that maintains a buffer of recent readings per customer, computes features when the window is full, and scores each window through the trained VAE."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Inject synthetic anomalies as model-independent ground truth ---\n",
    "np.random.seed(42)\n",
    "n_total = len(X_scaled)\n",
    "y_true = np.zeros(n_total, dtype=int)\n",
    "\n",
    "# Select 2% of indices to inject anomalies\n",
    "n_inject = int(n_total * 0.02)\n",
    "inject_idx = np.random.choice(n_total, size=n_inject, replace=False)\n",
    "y_true[inject_idx] = 1\n",
    "\n",
    "# Create a modified copy for scoring (leave training data unchanged)\n",
    "X_eval = X_scaled.copy()\n",
    "\n",
    "# Three anomaly types with known signatures:\n",
    "for i, idx in enumerate(inject_idx):\n",
    "    anomaly_type = i % 3\n",
    "    if anomaly_type == 0:  # voltage sag: mean drops 3+ std\n",
    "        X_eval[idx, 0] -= np.random.uniform(3.0, 5.0)\n",
    "    elif anomaly_type == 1:  # voltage spike: mean rises 3+ std\n",
    "        X_eval[idx, 0] += np.random.uniform(3.0, 5.0)\n",
    "    else:  # high variance: range and std increase\n",
    "        X_eval[idx, 1] += np.random.uniform(3.0, 6.0)  # voltage_std\n",
    "        X_eval[idx, 2] += np.random.uniform(3.0, 6.0)  # voltage_range\n",
    "\n",
    "eval_tensor = torch.FloatTensor(X_eval)\n",
    "print(f\"Injected {n_inject} synthetic anomalies ({n_inject/n_total*100:.1f}%)\")\n",
    "print(f\"  Types: {n_inject//3} sags, {n_inject//3} spikes, {n_inject - 2*(n_inject//3)} high-variance\")\n",
    "\n",
    "# --- Train basic autoencoder (matched architecture to VAE for fair comparison) ---\n",
    "# AE bottleneck = 3 dimensions (same as VAE latent_dim) to control for capacity\n",
    "class BasicAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 16),  nn.ReLU(),\n",
    "            nn.Linear(16, 3),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),  nn.ReLU(),\n",
    "            nn.Linear(16, 32), nn.ReLU(),\n",
    "            nn.Linear(32, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "ae = BasicAutoencoder(input_dim)\n",
    "ae_optimizer = torch.optim.Adam(ae.parameters(), lr=0.001)\n",
    "ae_criterion = nn.MSELoss()\n",
    "\n",
    "# Train basic AE for 50 epochs on same normal data\n",
    "ae_loader = DataLoader(TensorDataset(train_tensor, train_tensor),\n",
    "                        batch_size=128, shuffle=True)\n",
    "for epoch in range(50):\n",
    "    ae.train()\n",
    "    for bx, by in ae_loader:\n",
    "        loss = ae_criterion(ae(bx), by)\n",
    "        ae_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "# Score with basic autoencoder on synthetic-injected data\n",
    "ae.eval()\n",
    "with torch.no_grad():\n",
    "    ae_recon = ae(eval_tensor)\n",
    "    ae_error = torch.mean((eval_tensor - ae_recon) ** 2, dim=1).numpy()\n",
    "\n",
    "# Score with Isolation Forest on synthetic-injected data\n",
    "iso_scores = -iso_forest.decision_function(X_eval)  # negate so higher = more anomalous\n",
    "\n",
    "# Score with VAE on synthetic-injected data\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    mu, log_var = vae.encode(eval_tensor)\n",
    "    z = vae.reparameterize(mu, log_var)\n",
    "    recon = vae.decode(z)\n",
    "    recon_err = torch.mean((eval_tensor - recon) ** 2, dim=1)\n",
    "    kl_div = -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1)\n",
    "    vae_elbo_scores = (recon_err + kl_div).numpy()\n",
    "\n",
    "# Ground truth: synthetic anomaly labels (y_true defined above)\n",
    "\n",
    "# Precision-recall curves for each method\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "methods = {\n",
    "    \"Isolation Forest\":    iso_scores,\n",
    "    \"Basic Autoencoder\":   ae_error,\n",
    "    \"VAE (ELBO Score)\":    vae_elbo_scores,\n",
    "}\n",
    "colors = [\"#718096\", \"#D69E2E\", \"#2D6A7A\"]\n",
    "\n",
    "for (name, scores), color in zip(methods.items(), colors):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, scores)\n",
    "    ap = average_precision_score(y_true, scores)\n",
    "    ax.plot(recall, precision, label=f\"{name} (AP={ap:.3f})\",\n",
    "            color=color, linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.set_title(\"Precision-Recall: Anomaly Detection Methods Compared\")\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Adaptive Thresholding\n",
    "\n",
    "A fixed threshold works in stable conditions, but voltage patterns change with the seasons. In summer, air conditioning loads cause higher voltage variance\u2014what looks anomalous in January might be perfectly normal in July. An adaptive threshold uses a rolling percentile of recent scores to set the alert level, automatically adjusting to seasonal patterns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Score distributions: normal vs anomalous for each method\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (name, scores) in zip(axes, methods.items()):\n",
    "    normal_scores  = scores[y_true == 0]\n",
    "    anomaly_scores = scores[y_true == 1]\n",
    "\n",
    "    ax.hist(normal_scores, bins=80, alpha=0.6, color=\"#5FCCDB\",\n",
    "           label=\"Normal\", density=True)\n",
    "    ax.hist(anomaly_scores, bins=30, alpha=0.7, color=\"red\",\n",
    "           label=\"Anomaly\", density=True)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"Anomaly Score\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Score Distributions: Normal vs Anomalous Points\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Real-Time Operation\n",
    "\n",
    "Now we replay one week of AMI data through the streaming pipeline, simulating what would happen if this system were running in SP&L's control room. We then overlay actual outage events to see how our detections correlate with real grid problems."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load weather data for seasonal context\n",
    "weather = load_weather_data()\n",
    "\n",
    "# Load outage events for validation later\n",
    "outages = load_outage_history()\n",
    "\n",
    "print(f\"Weather records:  {len(weather):,}\")\n",
    "print(f\"Outage events:   {len(outages):,}\")\n",
    "print(f\"Customers:       {ami['customer_id'].nunique():,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence and Hyperparameter Justification\n",
    "\n",
    "For deployment, you need to save both the VAE weights and the scaler that was fit on training data. Without the scaler, new incoming data cannot be normalized consistently."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "class StreamingAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Sliding window anomaly detector for AMI voltage data.\n",
    "    Maintains a 1-hour buffer per customer and scores with a trained VAE.\n",
    "    \"\"\"\n",
    "    def __init__(self, vae_model, scaler, feature_cols,\n",
    "                 window_size=4, threshold=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vae_model:    trained VoltageVAE instance\n",
    "            scaler:       fitted StandardScaler\n",
    "            feature_cols: list of feature column names\n",
    "            window_size:  number of readings per window (4 = 1 hour at 15-min intervals)\n",
    "            threshold:    fixed ELBO threshold (None = use adaptive)\n",
    "        \"\"\"\n",
    "        self.vae = vae_model\n",
    "        self.vae.eval()\n",
    "        self.scaler = scaler\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.fixed_threshold = threshold\n",
    "\n",
    "        # Per-customer sliding window buffers\n",
    "        self.buffers = defaultdict(\n",
    "            lambda: deque(maxlen=window_size)\n",
    "        )\n",
    "\n",
    "        # Detection log\n",
    "        self.detections = []\n",
    "        self.all_scores = []\n",
    "\n",
    "    def ingest(self, customer_id, timestamp, voltage, energy_kwh):\n",
    "        \"\"\"Process a single AMI reading.\"\"\"\n",
    "        self.buffers[customer_id].append({\n",
    "            \"timestamp\": timestamp,\n",
    "            \"voltage_v\": voltage,\n",
    "            \"energy_kwh\": energy_kwh,\n",
    "        })\n",
    "\n",
    "        # Only score when we have a full window\n",
    "        if len(self.buffers[customer_id]) return None\n",
    "\n",
    "        # Extract features from the window\n",
    "        features = self._extract_features(customer_id)\n",
    "        score = self._score(features)\n",
    "\n",
    "        self.all_scores.append({\n",
    "            \"customer_id\": customer_id,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"elbo_score\": score,\n",
    "        })\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _extract_features(self, customer_id):\n",
    "        \"\"\"Compute hourly statistics from the sliding window.\"\"\"\n",
    "        readings = list(self.buffers[customer_id])\n",
    "        voltages = np.array([r[\"voltage_v\"] for r in readings])\n",
    "        consumption = sum(r[\"energy_kwh\"] for r in readings)\n",
    "\n",
    "        return {\n",
    "            \"voltage_mean\":     voltages.mean(),\n",
    "            \"voltage_std\":      voltages.std(),\n",
    "            \"voltage_range\":    voltages.max() - voltages.min(),\n",
    "            \"voltage_min\":      voltages.min(),\n",
    "            \"voltage_max\":      voltages.max(),\n",
    "            \"energy_kwh\":  consumption,\n",
    "        }\n",
    "\n",
    "    def _score(self, features):\n",
    "        \"\"\"Run features through the VAE and return ELBO score.\"\"\"\n",
    "        x = np.array([[features[c] for c in self.feature_cols]])\n",
    "        x_scaled = self.scaler.transform(x)\n",
    "        x_tensor = torch.FloatTensor(x_scaled)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_recon, mu, log_var = self.vae(x_tensor)\n",
    "            recon = F.mse_loss(x_recon, x_tensor, reduction=\"none\").mean(dim=1)\n",
    "            kl = -0.5 * torch.sum(\n",
    "                1 + log_var - mu.pow(2) - log_var.exp(), dim=1\n",
    "            )\n",
    "            score = (recon + kl).item()\n",
    "\n",
    "        return score\n",
    "\n",
    "# Create the detector with our trained VAE\n",
    "detector = StreamingAnomalyDetector(\n",
    "    vae_model=vae,\n",
    "    scaler=scaler,\n",
    "    feature_cols=feature_cols,\n",
    "    window_size=4,\n",
    "    threshold=elbo_threshold,\n",
    ")\n",
    "\n",
    "print(\"Streaming detector initialized.\")\n",
    "print(f\"  Window size:     {detector.window_size} readings (1 hour)\")\n",
    "print(f\"  Fixed threshold: {elbo_threshold:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Built and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class AdaptiveThreshold:\n",
    "    \"\"\"\n",
    "    Rolling percentile threshold that adapts to seasonal patterns.\n",
    "    Maintains a window of recent scores and computes the threshold\n",
    "    as the Nth percentile of that window.\n",
    "    \"\"\"\n",
    "    def __init__(self, window_hours=168, percentile=99.0,\n",
    "                 min_samples=24, fallback_threshold=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            window_hours:      lookback window for computing percentile (168 = 1 week)\n",
    "            percentile:        percentile to use as threshold (99 = flag top 1%)\n",
    "            min_samples:       minimum scores before adaptive threshold activates\n",
    "            fallback_threshold: fixed threshold used until min_samples is reached\n",
    "        \"\"\"\n",
    "        self.window_size = window_hours\n",
    "        self.percentile = percentile\n",
    "        self.min_samples = min_samples\n",
    "        self.fallback = fallback_threshold\n",
    "\n",
    "        self.score_history = deque(maxlen=window_hours)\n",
    "\n",
    "    def update_and_check(self, score):\n",
    "        \"\"\"Add a score and return (is_anomaly, current_threshold).\"\"\"\n",
    "        self.score_history.append(score)\n",
    "\n",
    "        if len(self.score_history) else:\n",
    "            threshold = np.percentile(\n",
    "                list(self.score_history), self.percentile\n",
    "            )\n",
    "\n",
    "        is_anomaly = score > threshold\n",
    "        return is_anomaly, threshold\n",
    "\n",
    "# Upgrade the detector with adaptive thresholding\n",
    "class AdaptiveStreamingDetector(StreamingAnomalyDetector):\n",
    "    \"\"\"Streaming detector with per-customer adaptive thresholds.\"\"\"\n",
    "\n",
    "    def __init__(self, vae_model, scaler, feature_cols,\n",
    "                 window_size=4, lookback_hours=168, percentile=99.0):\n",
    "        super().__init__(vae_model, scaler, feature_cols, window_size)\n",
    "        self.lookback_hours = lookback_hours\n",
    "        self.percentile = percentile\n",
    "\n",
    "        # Per-customer adaptive thresholds\n",
    "        self.adaptive_thresholds = defaultdict(\n",
    "            lambda: AdaptiveThreshold(\n",
    "                window_hours=lookback_hours,\n",
    "                percentile=percentile,\n",
    "                fallback_threshold=elbo_threshold,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def ingest(self, customer_id, timestamp, voltage, energy_kwh):\n",
    "        \"\"\"Process a reading with adaptive thresholding.\"\"\"\n",
    "        score = super().ingest(customer_id, timestamp, voltage, energy_kwh)\n",
    "\n",
    "        if score is None:\n",
    "            return None\n",
    "\n",
    "        # Check against adaptive threshold for this customer\n",
    "        is_anomaly, threshold = self.adaptive_thresholds[customer_id].update_and_check(score)\n",
    "\n",
    "        if is_anomaly:\n",
    "            self.detections.append({\n",
    "                \"customer_id\":   customer_id,\n",
    "                \"timestamp\":  timestamp,\n",
    "                \"elbo_score\": score,\n",
    "                \"threshold\":  threshold,\n",
    "            })\n",
    "\n",
    "        return {\"score\": score, \"threshold\": threshold, \"is_anomaly\": is_anomaly}\n",
    "\n",
    "# Create adaptive detector\n",
    "adaptive_detector = AdaptiveStreamingDetector(\n",
    "    vae_model=vae,\n",
    "    scaler=scaler,\n",
    "    feature_cols=feature_cols,\n",
    "    window_size=4,\n",
    "    lookback_hours=168,    # 1-week rolling window\n",
    "    percentile=99.0,       # flag top 1%\n",
    ")\n",
    "\n",
    "print(\"Adaptive streaming detector ready.\")\n",
    "print(f\"  Lookback window: {adaptive_detector.lookback_hours} hours (1 week)\")\n",
    "print(f\"  Percentile:      {adaptive_detector.percentile}th\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select one week of data for simulation (matches the AMI data range)\n",
    "sim_start = \"2024-07-15\"   # Monday, mid-summer (high load)\n",
    "sim_end   = \"2024-07-22\"\n",
    "\n",
    "sim_data = ami[(ami[\"timestamp\"] >= sim_start) &\n",
    "               (ami[\"timestamp\"] sort_values(\"timestamp\")\n",
    "\n",
    "print(f\"Simulation period: {sim_start} to {sim_end}\")\n",
    "print(f\"Readings to process: {len(sim_data):,}\")\n",
    "print(f\"Customers:     {sim_data['customer_id'].nunique():,}\")\n",
    "\n",
    "# Replay data through the adaptive detector\n",
    "# NOTE: We use itertuples() instead of iterrows() for ~5-10x speed improvement.\n",
    "# For production-scale streaming, consider a vectorized batch approach or a\n",
    "# dedicated streaming framework (Kafka + Flink, or pandas pipe with groupby).\n",
    "results = []\n",
    "for row in sim_data.itertuples(index=False):\n",
    "    result = adaptive_detector.ingest(\n",
    "        customer_id=row.customer_id,\n",
    "        timestamp=row.timestamp,\n",
    "        voltage=row.voltage_v,\n",
    "        energy_kwh=row.energy_kwh,\n",
    "    )\n",
    "    if result is not None:\n",
    "        results.append({\n",
    "            \"customer_id\":   row.customer_id,\n",
    "            \"timestamp\":  row.timestamp,\n",
    "            **result,\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "detections_df = pd.DataFrame(adaptive_detector.detections)\n",
    "\n",
    "print(f\"\\nWindows scored:    {len(results_df):,}\")\n",
    "print(f\"Anomalies flagged: {len(detections_df):,}\")\n",
    "print(f\"Alert rate:        {len(detections_df)/max(len(results_df),1)*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load outage events for the same week\n",
    "week_outages = outages[\n",
    "    (outages[\"fault_detected\"] >= sim_start) &\n",
    "    (outages[\"fault_detected\"] print(f\"Actual outage events this week: {len(week_outages)}\")\n",
    "print(week_outages[[\"fault_detected\", \"feeder_id\", \"cause_code\",\n",
    "                    \"affected_customers\"]].to_string(index=False))\n",
    "\n",
    "# Timeline visualization: scores and detections overlaid with outage events\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Top: ELBO scores over time (aggregate across customers per hour)\n",
    "results_df[\"hour\"] = results_df[\"timestamp\"].dt.floor(\"h\")\n",
    "hourly_max_score = results_df.groupby(\"hour\")[\"score\"].max()\n",
    "hourly_avg_score = results_df.groupby(\"hour\")[\"score\"].mean()\n",
    "\n",
    "ax1.fill_between(hourly_avg_score.index, hourly_avg_score.values,\n",
    "                  alpha=0.3, color=\"#5FCCDB\", label=\"Avg ELBO score\")\n",
    "ax1.plot(hourly_max_score.index, hourly_max_score.values,\n",
    "         color=\"#2D6A7A\", linewidth=0.8, alpha=0.7, label=\"Max ELBO score\")\n",
    "\n",
    "# Mark outage events\n",
    "for _, outage in week_outages.iterrows():\n",
    "    ax1.axvline(x=outage[\"fault_detected\"], color=\"red\", linestyle=\"--\",\n",
    "                alpha=0.8, linewidth=1.5)\n",
    "# Add a legend entry for outage lines\n",
    "ax1.plot([], [], color=\"red\", linestyle=\"--\", linewidth=1.5, label=\"Outage event\")\n",
    "\n",
    "ax1.set_ylabel(\"ELBO Anomaly Score\")\n",
    "ax1.set_title(\"Streaming VAE Anomaly Scores \u2014 July 15-21, 2024\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "\n",
    "# Bottom: detection timeline (heatmap of flagged customers)\n",
    "if len(detections_df) > 0:\n",
    "    det_hourly = detections_df.groupby(\n",
    "        detections_df[\"timestamp\"].dt.floor(\"h\")\n",
    "    ).size()\n",
    "    ax2.bar(det_hourly.index, det_hourly.values,\n",
    "           width=0.04, color=\"#E53E3E\", alpha=0.8, label=\"Anomaly detections\")\n",
    "\n",
    "for _, outage in week_outages.iterrows():\n",
    "    ax2.axvline(x=outage[\"fault_detected\"], color=\"red\", linestyle=\"--\",\n",
    "                alpha=0.8, linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel(\"Time\")\n",
    "ax2.set_ylabel(\"Detections per Hour\")\n",
    "ax2.set_title(\"Anomaly Detection Timeline vs Actual Outages\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Correlation analysis: how many outages had prior anomaly detections?\n",
    "lead_time_hours = 2  # how far back to look for preceding detections\n",
    "\n",
    "if len(detections_df) > 0:\n",
    "    outage_hits = 0\n",
    "    outage_details = []\n",
    "\n",
    "    for _, outage in week_outages.iterrows():\n",
    "        fault_time = outage[\"fault_detected\"]\n",
    "        window_start = fault_time - pd.Timedelta(hours=lead_time_hours)\n",
    "\n",
    "        # Check if any detections occurred in the lead-time window\n",
    "        prior_detections = detections_df[\n",
    "            (detections_df[\"timestamp\"] >= window_start) &\n",
    "            (detections_df[\"timestamp\"] len(prior_detections) > 0\n",
    "        outage_hits += int(hit)\n",
    "        outage_details.append({\n",
    "            \"fault_time\":     fault_time,\n",
    "            \"cause_code\":          outage[\"cause_code\"],\n",
    "            \"prior_alerts\":   len(prior_detections),\n",
    "            \"detected\":       hit,\n",
    "        })\n",
    "\n",
    "    correlation_df = pd.DataFrame(outage_details)\n",
    "    hit_rate = outage_hits / len(week_outages) * 100\n",
    "\n",
    "    print(f\"\\nOutage correlation analysis ({lead_time_hours}h lead time):\")\n",
    "    print(f\"  Outages with prior anomaly detection: {outage_hits}/{len(week_outages)} ({hit_rate:.1f}%)\")\n",
    "    print(f\"\\nDetails:\")\n",
    "    print(correlation_df.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained VAE\n",
    "torch.save(vae.state_dict(), \"voltage_vae.pt\")\n",
    "\n",
    "# Save the fitted scaler (needed to normalize new data consistently)\n",
    "joblib.dump(scaler, \"voltage_scaler.pkl\")\n",
    "\n",
    "# Load:\n",
    "# vae.load_state_dict(torch.load(\"voltage_vae.pt\"))\n",
    "# scaler = joblib.load(\"voltage_scaler.pkl\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}