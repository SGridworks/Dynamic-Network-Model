{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "name": "06-volt-var-optimization.ipynb"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volt-VAR Optimization with Reinforcement Learning\n",
    "\n",
    "From the [Sisyphean Gridworks ML Playground](https://sgridworks.com/ml-playground/guides/06-volt-var-optimization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Clone the repository and install dependencies. Run this cell first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone https://github.com/SGridworks/Dynamic-Network-Model.git 2>/dev/null || echo 'Already cloned'\n",
    "%cd Dynamic-Network-Model\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm pyarrow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Voltage Profile"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load SP&L datasets\n",
    "from demo_data.load_demo_data import (\n",
    "    load_load_profiles, load_network_edges, load_network_nodes\n",
    ")\n",
    "\n",
    "load_profiles = load_load_profiles()\n",
    "edges = load_network_edges()\n",
    "nodes = load_network_nodes()\n",
    "\n",
    "# Pick Feeder FDR-0001 and look at voltage profiles\n",
    "feeder_load = load_profiles[load_profiles[\"feeder_id\"] == \"FDR-0001\"].copy()\n",
    "\n",
    "# The load_profiles already contain voltage_pu measurements\n",
    "# Plot voltage variation over a representative day\n",
    "one_day = feeder_load.head(96)  # 96 intervals = 24 hours at 15-min\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(range(len(one_day)), one_day[\"voltage_pu\"], \"o-\", color=\"#5FCCDB\", markersize=4)\n",
    "ax.axhline(y=1.05, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Upper limit\")\n",
    "ax.axhline(y=0.95, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Lower limit\")\n",
    "ax.axhspan(0.95, 1.05, alpha=0.1, color=\"green\", label=\"ANSI range\")\n",
    "ax.set_xlabel(\"15-Minute Interval\")\n",
    "ax.set_ylabel(\"Voltage (p.u.)\")\n",
    "ax.set_title(\"Voltage Profile \u2014 Feeder FDR-0001 (One Day)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Rule-Based Controller\n",
    "\n",
    "Before using ML, build a simple rule: \"If the voltage at the end of the feeder drops below 0.97 p.u., switch on a capacitor bank. If it rises above 1.03 p.u., switch it off.\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def rule_based_vvo(voltage_pu, cap_on):\n",
    "    \"\"\"Simple rule-based capacitor control.\"\"\"\n",
    "    if voltage_pu 0.97 and not cap_on:\n",
    "        return True   # switch ON to boost voltage\n",
    "    elif voltage_pu > 1.03 and cap_on:\n",
    "        return False  # switch OFF to reduce voltage\n",
    "    return cap_on\n",
    "\n",
    "# Simulate 24 hours using actual voltage profile data\n",
    "# We use the voltage_pu column and simulate capacitor effect\n",
    "day_data = feeder_load.head(96).copy()\n",
    "\n",
    "cap_on = False\n",
    "rule_results = []\n",
    "\n",
    "for i, (_, row) in enumerate(day_data.iterrows()):\n",
    "    v = row[\"voltage_pu\"]\n",
    "\n",
    "    # Simulate capacitor effect: +0.02 p.u. when on\n",
    "    if cap_on:\n",
    "        v += 0.02\n",
    "\n",
    "    cap_on = rule_based_vvo(v, cap_on)\n",
    "    rule_results.append({\"interval\": i, \"voltage_pu\": v, \"cap_on\": cap_on})\n",
    "\n",
    "rule_df = pd.DataFrame(rule_results)\n",
    "print(rule_df.head(24))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "The rule-based controller works, but it requires you to hand-pick the voltage thresholds and logic. What if the agent could learn the best strategy by itself? That's what reinforcement learning (RL) does."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class VoltVAREnv:\n",
    "    \"\"\"Simple Volt-VAR environment for Q-learning.\"\"\"\n",
    "\n",
    "    def __init__(self, load_data):\n",
    "        self.load_data = load_data.reset_index(drop=True)\n",
    "        self.step_idx = 0\n",
    "        self.cap_on = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_idx = 0\n",
    "        self.cap_on = False\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_voltage(self):\n",
    "        row = self.load_data.iloc[self.step_idx % len(self.load_data)]\n",
    "        v = row[\"voltage_pu\"]\n",
    "        if self.cap_on:\n",
    "            v += 0.02  # capacitor boost\n",
    "        return v\n",
    "\n",
    "    def _get_state(self):\n",
    "        v = self._get_voltage()\n",
    "        if v 0.95:   bucket = 0\n",
    "        elif v 0.97: bucket = 1\n",
    "        elif v 1.00: bucket = 2\n",
    "        elif v 1.03: bucket = 3\n",
    "        else:           bucket = 4\n",
    "        return (bucket, int(self.cap_on))\n",
    "\n",
    "    def step(self, action):\n",
    "        self.cap_on = bool(action)\n",
    "        voltage = self._get_voltage()\n",
    "\n",
    "        if 0.95 1.05:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = -10.0\n",
    "        reward += max(0, 1 - abs(voltage - 1.0) * 20)\n",
    "\n",
    "        self.step_idx += 1\n",
    "        done = self.step_idx >= len(self.load_data)\n",
    "        return self._get_state(), reward, done, {\"voltage\": voltage}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RL Environment\n",
    "\n",
    "Now let's set up a reinforcement learning environment. The agent observes the current voltage and decides whether to switch the capacitor on or off. It gets a positive reward when voltage is within the ANSI range and a negative reward (penalty) when it's outside."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Q-table: a lookup table with one entry per (state, action) pair\n",
    "# Dimensions: 5 voltage buckets x 2 cap states x 2 possible actions\n",
    "# Initialized to zeros \u2014 the agent starts with no knowledge\n",
    "q_table = np.zeros((5, 2, 2))\n",
    "\n",
    "# Hyperparameters \u2014 these control how the agent learns\n",
    "alpha = 0.1      # learning rate: how much to update Q-values each step\n",
    "gamma = 0.95     # discount factor: how much to value future vs. immediate rewards\n",
    "epsilon = 1.0    # exploration rate: start with 100% random actions\n",
    "epsilon_min = 0.05   # never stop exploring completely (5% random)\n",
    "epsilon_decay = 0.995 # multiply epsilon by this after each episode\n",
    "n_episodes = 100  # number of training runs through the day's data\n",
    "\n",
    "env = VoltVAREnv(day_data)\n",
    "episode_rewards = []\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    state = env.reset()  # start a new episode (reset to hour 0)\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # Epsilon-greedy: explore randomly or exploit best-known action\n",
    "        if np.random.random() randint(2)  # random: 0=cap off, 1=cap on\n",
    "        else:\n",
    "            action = np.argmax(q_table[state[0], state[1]])  # pick best action from Q-table\n",
    "\n",
    "        # Take the action and observe what happens\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # THE Q-LEARNING UPDATE EQUATION:\n",
    "        # Q(s,a) = Q(s,a) + \u03b1 * [reward + \u03b3 * max_a' Q(s',a') - Q(s,a)]\n",
    "        #\n",
    "        # In plain English: adjust the old estimate toward the\n",
    "        # actual reward received PLUS the discounted value of the\n",
    "        # best action in the next state.\n",
    "        old_q = q_table[state[0], state[1], action]         # current Q-value estimate\n",
    "        best_next = np.max(q_table[next_state[0], next_state[1]]) # best future value\n",
    "        q_table[state[0], state[1], action] = old_q + alpha * (\n",
    "            reward + gamma * best_next - old_q  # temporal difference error\n",
    "        )\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon: explore less as the agent learns more\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    if (ep + 1) % 20 == 0:\n",
    "        print(f\"Episode {ep+1:>3}/{n_episodes}  \"\n",
    "              f\"Reward: {total_reward:.1f}  Epsilon: {epsilon:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot training progress\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(episode_rewards, color=\"#5FCCDB\", alpha=0.6)\n",
    "ax.plot(pd.Series(episode_rewards).rolling(10).mean(),\n",
    "       color=\"#1C4855\", linewidth=2, label=\"10-episode avg\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Total Reward\")\n",
    "ax.set_title(\"Q-Learning Training Progress\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Run the trained agent for one day and compare with rule-based\n",
    "env = VoltVAREnv(day_data)\n",
    "state = env.reset()\n",
    "rl_results = []\n",
    "\n",
    "while True:\n",
    "    action = np.argmax(q_table[state[0], state[1]])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    rl_results.append({\"interval\": env.step_idx - 1, \"voltage_pu\": info[\"voltage\"],\n",
    "                       \"cap_on\": bool(action)})\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "rl_df = pd.DataFrame(rl_results)\n",
    "\n",
    "# Side-by-side comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "ax1.plot(rule_df[\"interval\"], rule_df[\"voltage_pu\"], \"o-\", color=\"#2D6A7A\")\n",
    "ax1.axhspan(0.95, 1.05, alpha=0.1, color=\"green\")\n",
    "ax1.set_title(\"Rule-Based Controller\")\n",
    "ax1.set_xlabel(\"15-Minute Interval\")\n",
    "ax1.set_ylabel(\"Voltage (p.u.)\")\n",
    "\n",
    "ax2.plot(rl_df[\"interval\"], rl_df[\"voltage_pu\"], \"o-\", color=\"#5FCCDB\")\n",
    "ax2.axhspan(0.95, 1.05, alpha=0.1, color=\"green\")\n",
    "ax2.set_title(\"Q-Learning Controller\")\n",
    "ax2.set_xlabel(\"15-Minute Interval\")\n",
    "\n",
    "plt.suptitle(\"VVO Controller Comparison\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Compare Both Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Built and Next Steps"
   ]
  }
 ]
}