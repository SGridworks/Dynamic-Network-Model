{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "name": "10-advanced-load-forecasting.ipynb"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Multi-Step Load Forecasting with PyTorch\n",
    "\n",
    "From the [Sisyphean Gridworks ML Playground](https://sgridworks.com/ml-playground/guides/10-advanced-load-forecasting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Clone the repository and install dependencies. Run this cell first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone https://github.com/SGridworks/Dynamic-Network-Model.git 2>/dev/null || echo 'Already cloned'\n",
    "%cd Dynamic-Network-Model\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm pyarrow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Data\n",
    "\n",
    "We start with the same SP&L load profile and weather data from Guide 02, but this time we will keep the raw 15-minute series intact rather than flattening it into tabular features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from demo_data.load_demo_data import load_load_profiles, load_weather_data\n",
    "\n",
    "# Load 15-minute feeder load profiles\n",
    "load = load_load_profiles()\n",
    "\n",
    "# Load hourly weather\n",
    "weather = load_weather_data()\n",
    "\n",
    "# Focus on Feeder FDR-0001 (same as Guide 02)\n",
    "feeder = load[load[\"feeder_id\"] == \"FDR-0001\"].copy()\n",
    "feeder = feeder.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# Merge weather data (weather is hourly; load is 15-min)\n",
    "weather[\"timestamp\"] = pd.to_datetime(weather[\"timestamp\"])\n",
    "df = feeder.merge(\n",
    "    weather[[\"timestamp\", \"temperature\", \"humidity\", \"wind_speed\"]],\n",
    "    on=\"timestamp\", how=\"left\"\n",
    ")\n",
    "df = df.dropna(subset=[\"temperature\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total 15-minute records: {len(df):,}\")\n",
    "print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Temporal Patterns\n",
    "\n",
    "Before building any model, it is critical to understand the patterns your LSTM needs to capture. Electricity load exhibits strong daily, weekly, and seasonal cycles that a well-trained LSTM should learn to reproduce."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Add time features for exploration\n",
    "df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "df[\"day_of_week\"] = df[\"timestamp\"].dt.dayofweek\n",
    "df[\"month\"] = df[\"timestamp\"].dt.month\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Daily cycle: average load by hour\n",
    "df.groupby(\"hour\")[\"load_mw\"].mean().plot(\n",
    "    ax=axes[0, 0], color=\"#2D6A7A\", linewidth=2)\n",
    "axes[0, 0].set_title(\"Daily Cycle: Average Load by Hour\")\n",
    "axes[0, 0].set_ylabel(\"Load (MW)\")\n",
    "\n",
    "# Weekly cycle: average load by day of week\n",
    "day_names = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "weekly = df.groupby(\"day_of_week\")[\"load_mw\"].mean()\n",
    "axes[0, 1].bar(day_names, weekly.values, color=\"#5FCCDB\")\n",
    "axes[0, 1].set_title(\"Weekly Cycle: Average Load by Day\")\n",
    "axes[0, 1].set_ylabel(\"Load (MW)\")\n",
    "\n",
    "# Seasonal cycle: average load by month\n",
    "df.groupby(\"month\")[\"load_mw\"].mean().plot(\n",
    "    kind=\"bar\", ax=axes[1, 0], color=\"#2D6A7A\")\n",
    "axes[1, 0].set_title(\"Seasonal Cycle: Average Load by Month\")\n",
    "axes[1, 0].set_ylabel(\"Load (MW)\")\n",
    "\n",
    "# Temperature vs load scatter\n",
    "axes[1, 1].scatter(df[\"temperature\"], df[\"load_mw\"],\n",
    "                    alpha=0.02, s=1, color=\"#2D6A7A\")\n",
    "axes[1, 1].set_title(\"Temperature vs Load (U-shaped)\")\n",
    "axes[1, 1].set_xlabel(\"Temperature (\u00b0F)\")\n",
    "axes[1, 1].set_ylabel(\"Load (MW)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sliding Window Sequences\n",
    "\n",
    "LSTMs consume fixed-length sequences. We use a sliding window: given the past 168 hours (1 week) of load data, predict the next 24 hours of load. Each window slides forward by one hour to create overlapping training samples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "INPUT_HOURS = 168   # 7 days of history as input\n",
    "OUTPUT_HOURS = 24   # predict 24 hours ahead\n",
    "\n",
    "# Extract the target series\n",
    "load_series = df[\"load_mw\"].values.reshape(-1, 1)\n",
    "\n",
    "# IMPORTANT: fit scaler on training data only (first 70%) to avoid data leakage\n",
    "train_end_idx = int(len(df) * 0.7)\n",
    "scaler_load = StandardScaler()\n",
    "scaler_load.fit(load_series[:train_end_idx])\n",
    "load_scaled = scaler_load.transform(load_series).flatten()\n",
    "\n",
    "print(f\"Load mean: {scaler_load.mean_[0]:.3f} MW\")\n",
    "print(f\"Load std:  {scaler_load.scale_[0]:.3f} MW\")\n",
    "\n",
    "# Build sliding window sequences\n",
    "def create_sequences(data, input_len, output_len):\n",
    "    \"\"\"Create input/output pairs using a sliding window.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_len - output_len + 1):\n",
    "        X.append(data[i : i + input_len])\n",
    "        y.append(data[i + input_len : i + input_len + output_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_all, y_all = create_sequences(load_scaled, INPUT_HOURS, OUTPUT_HOURS)\n",
    "\n",
    "print(f\"\\nTotal sequences: {len(X_all):,}\")\n",
    "print(f\"Input shape:  {X_all.shape}  (samples, 168 timesteps)\")\n",
    "print(f\"Output shape: {y_all.shape}  (samples, 24 timesteps)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chronological Train/Validation/Test Split\n",
    "\n",
    "Time-series data must be split chronologically. We use the first 70% of sequences for training, the next 15% for validation (hyperparameter tuning), and the final 15% for testing. No shuffling\u2014the model never sees the future during training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Chronological split: 70% train, 15% val, 15% test\n",
    "n = len(X_all)\n",
    "train_end = int(n * 0.70)\n",
    "val_end   = int(n * 0.85)\n",
    "\n",
    "X_train, y_train = X_all[:train_end],        y_all[:train_end]\n",
    "X_val,   y_val   = X_all[train_end:val_end],  y_all[train_end:val_end]\n",
    "X_test,  y_test  = X_all[val_end:],           y_all[val_end:]\n",
    "\n",
    "print(f\"Train: {len(X_train):,} sequences (first 70%)\")\n",
    "print(f\"Val:   {len(X_val):,} sequences (next 15%)\")\n",
    "print(f\"Test:  {len(X_test):,} sequences (final 15%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LSTM Model in PyTorch\n",
    "\n",
    "Now we define the LSTM architecture. The model processes the 168-hour input sequence one timestep at a time, building up a hidden state that summarizes the history. The final hidden state is then passed through fully connected layers to produce the 24-hour forecast."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PyTorch Dataset for batching\n",
    "class LoadDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # Reshape X to (samples, timesteps, features=1)\n",
    "        self.X = torch.FloatTensor(X).unsqueeze(-1)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "train_ds = LoadDataset(X_train, y_train)\n",
    "val_ds   = LoadDataset(X_val, y_val)\n",
    "test_ds  = LoadDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the LSTM\n",
    "\n",
    "We train using MSE loss with the Adam optimizer and an exponential learning rate scheduler. Early stopping on validation loss prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the LSTM model\n",
    "class LoadForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128,\n",
    "                 num_layers=2, output_size=24, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layers: process the input sequence\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Fully connected layers: map hidden state to forecast\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 168, 1)\n",
    "        # lstm_out shape: (batch, 168, hidden_size)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # Use the last hidden state to produce the forecast\n",
    "        last_hidden = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
    "        forecast = self.fc(last_hidden)    # (batch, 24)\n",
    "        return forecast\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LoadForecaster().to(device)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Step Evaluation: MAPE, RMSE, and Visualization\n",
    "\n",
    "Now we evaluate the trained LSTM on the held-out test set (final 15%). Since the model outputs 24 values at once, we can assess accuracy at each forecast horizon (1 hour ahead, 2 hours ahead, ... 24 hours ahead)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training configuration\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "PATIENCE = 5  # early stopping patience\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# Training loop with early stopping\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item() * len(X_batch)\n",
    "\n",
    "    avg_train = epoch_train_loss / len(train_ds)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            epoch_val_loss += loss.item() * len(X_batch)\n",
    "\n",
    "    avg_val = epoch_val_loss / len(val_ds)\n",
    "\n",
    "    train_losses.append(avg_train)\n",
    "    val_losses.append(avg_val)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {avg_train:.6f} | Val Loss: {avg_val:.6f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val save(model.state_dict(), \"best_lstm_load.pt\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_lstm_load.pt\"))\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.6f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Weather Features as Exogenous Inputs\n",
    "\n",
    "So far our LSTM only sees historical load. But temperature is the single biggest driver of demand. Let's add weather data as additional input features alongside load. This transforms the LSTM from a univariate to a multivariate model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, label=\"Train Loss\", linewidth=2)\n",
    "ax.plot(val_losses, label=\"Validation Loss\", linewidth=2)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"MSE Loss (normalized)\")\n",
    "ax.set_title(\"LSTM Training and Validation Loss\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare LSTM to Gradient Boosting Baseline\n",
    "\n",
    "In Guide 02, you built a Gradient Boosting model that predicted one hour ahead with hand-crafted features. Let's rebuild that baseline and compare it to our LSTM models. Note that this is a \u201cdirect multi-output LSTM vs. single-step GB\u201d comparison\u2014the GB model produces one-step-ahead predictions using known lag features, while the LSTM forecasts all 24 hours simultaneously without iterative re-feeding. These represent different forecasting paradigms, and the MAPE numbers are not directly apples-to-apples. The GB number represents the best-case scenario for a one-step model; in an autoregressive 24-step rollout (where each prediction feeds into the next), the GB\u2019s error would compound and grow substantially."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate predictions on the test set\n",
    "model.eval()\n",
    "all_preds, all_actuals = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        preds = model(X_batch).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_actuals.append(y_batch.numpy())\n",
    "\n",
    "y_pred_scaled = np.concatenate(all_preds)\n",
    "y_true_scaled = np.concatenate(all_actuals)\n",
    "\n",
    "# Inverse-transform back to MW\n",
    "y_pred_mw = scaler_load.inverse_transform(\n",
    "    y_pred_scaled.reshape(-1, 1)).reshape(-1, OUTPUT_HOURS)\n",
    "y_true_mw = scaler_load.inverse_transform(\n",
    "    y_true_scaled.reshape(-1, 1)).reshape(-1, OUTPUT_HOURS)\n",
    "\n",
    "# Overall metrics\n",
    "mae  = mean_absolute_error(y_true_mw.flatten(), y_pred_mw.flatten())\n",
    "rmse = np.sqrt(mean_squared_error(y_true_mw.flatten(), y_pred_mw.flatten()))\n",
    "mape = np.mean(np.abs(\n",
    "    (y_true_mw.flatten() - y_pred_mw.flatten()) / y_true_mw.flatten()\n",
    ")) * 100\n",
    "\n",
    "print(f\"Test Set Metrics (all 24 horizons):\")\n",
    "print(f\"  MAE:  {mae:.4f} MW\")\n",
    "print(f\"  RMSE: {rmse:.4f} MW\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Mistakes in Time-Series ML\n",
    "\n",
    "Before wrapping up, let's review several common pitfalls that can quietly undermine your results. These mistakes are especially prevalent in time-series forecasting and are worth internalizing before moving to production."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Error by forecast horizon\n",
    "horizon_mae = []\n",
    "for h in range(OUTPUT_HOURS):\n",
    "    h_mae = mean_absolute_error(y_true_mw[:, h], y_pred_mw[:, h])\n",
    "    horizon_mae.append(h_mae)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(range(1, 25), horizon_mae, color=\"#5FCCDB\")\n",
    "ax.set_xlabel(\"Forecast Horizon (hours ahead)\")\n",
    "ax.set_ylabel(\"MAE (MW)\")\n",
    "ax.set_title(\"Forecast Error by Horizon: Accuracy Degrades Gracefully\")\n",
    "ax.set_xticks(range(1, 25))\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n1-hour ahead MAE:  {horizon_mae[0]:.4f} MW\")\n",
    "print(f\"12-hour ahead MAE: {horizon_mae[11]:.4f} MW\")\n",
    "print(f\"24-hour ahead MAE: {horizon_mae[23]:.4f} MW\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up and Next Steps\n",
    "\n",
    "You built a multi-step LSTM load forecasting system that predicts an entire 24-hour load curve in a single forward pass. Here's what you accomplished:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot a sample 24-hour forecast vs. actual\n",
    "sample_idx = 500  # pick a sample from the test set\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "hours = range(1, 25)\n",
    "ax.plot(hours, y_true_mw[sample_idx], \"o-\",\n",
    "        label=\"Actual\", linewidth=2, color=\"#2D6A7A\")\n",
    "ax.plot(hours, y_pred_mw[sample_idx], \"s--\",\n",
    "        label=\"LSTM Forecast\", linewidth=2, color=\"#5FCCDB\")\n",
    "ax.fill_between(hours,\n",
    "    y_pred_mw[sample_idx] - 2 * horizon_mae,\n",
    "    y_pred_mw[sample_idx] + 2 * horizon_mae,\n",
    "    alpha=0.15, color=\"#5FCCDB\", label=\"\u00b12 MAE band\")\n",
    "ax.set_xlabel(\"Hours Ahead\")\n",
    "ax.set_ylabel(\"Load (MW)\")\n",
    "ax.set_title(\"Sample 24-Hour Load Forecast vs. Actual\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build multivariate feature array: [load, temperature, humidity, wind_speed]\n",
    "features = df[[\"load_mw\", \"temperature\",\n",
    "               \"humidity\", \"wind_speed\"]].values\n",
    "\n",
    "# Fit on training data only (same boundary as univariate scaler)\n",
    "scaler_feat = StandardScaler()\n",
    "scaler_feat.fit(features[:train_end_idx])\n",
    "features_scaled = scaler_feat.transform(features)\n",
    "\n",
    "# Build sequences with multivariate input, univariate output\n",
    "def create_multivariate_sequences(features, targets, input_len, output_len):\n",
    "    \"\"\"\n",
    "    features: (N, num_features) - all input channels\n",
    "    targets:  (N,) - the load column only (already scaled)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - input_len - output_len + 1):\n",
    "        X.append(features[i : i + input_len])       # (168, 4)\n",
    "        y.append(targets[i + input_len : i + input_len + output_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Target is still just load (first column of scaled features)\n",
    "target_scaled = features_scaled[:, 0]\n",
    "\n",
    "X_mv, y_mv = create_multivariate_sequences(\n",
    "    features_scaled, target_scaled, INPUT_HOURS, OUTPUT_HOURS)\n",
    "\n",
    "print(f\"Multivariate input shape:  {X_mv.shape}\")\n",
    "print(f\"  (samples, 168 timesteps, 4 features)\")\n",
    "print(f\"Output shape: {y_mv.shape}\")\n",
    "\n",
    "# Same chronological split\n",
    "X_train_mv, y_train_mv = X_mv[:train_end],        y_mv[:train_end]\n",
    "X_val_mv,   y_val_mv   = X_mv[train_end:val_end], y_mv[train_end:val_end]\n",
    "X_test_mv,  y_test_mv  = X_mv[val_end:],          y_mv[val_end:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Updated Dataset for multivariate inputs\n",
    "class MultiVarLoadDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)  # already (samples, 168, 4)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create new DataLoaders\n",
    "train_mv_loader = DataLoader(\n",
    "    MultiVarLoadDataset(X_train_mv, y_train_mv),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_mv_loader = DataLoader(\n",
    "    MultiVarLoadDataset(X_val_mv, y_val_mv),\n",
    "    batch_size=BATCH_SIZE)\n",
    "test_mv_loader = DataLoader(\n",
    "    MultiVarLoadDataset(X_test_mv, y_test_mv),\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "# Train multivariate model (input_size=4 now)\n",
    "model_mv = LoadForecaster(input_size=4, hidden_size=128,\n",
    "                           num_layers=2, output_size=24).to(device)\n",
    "\n",
    "optimizer_mv = torch.optim.Adam(model_mv.parameters(), lr=LR)\n",
    "scheduler_mv = torch.optim.lr_scheduler.ExponentialLR(optimizer_mv, gamma=0.95)\n",
    "\n",
    "best_val_mv = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_mv.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X_b, y_b in train_mv_loader:\n",
    "        X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "        optimizer_mv.zero_grad()\n",
    "        loss = criterion(model_mv(X_b), y_b)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_mv.parameters(), 1.0)\n",
    "        optimizer_mv.step()\n",
    "        epoch_loss += loss.item() * len(X_b)\n",
    "\n",
    "    model_mv.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in val_mv_loader:\n",
    "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "            val_loss += criterion(model_mv(X_b), y_b).item() * len(X_b)\n",
    "\n",
    "    avg_val = val_loss / len(X_val_mv)\n",
    "    scheduler_mv.step()\n",
    "\n",
    "    if avg_val save(model_mv.state_dict(), \"best_lstm_mv.pt\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d} | Val Loss: {avg_val:.6f}\")\n",
    "\n",
    "model_mv.load_state_dict(torch.load(\"best_lstm_mv.pt\"))\n",
    "\n",
    "# Evaluate multivariate model\n",
    "all_preds_mv = []\n",
    "with torch.no_grad():\n",
    "    for X_b, _ in test_mv_loader:\n",
    "        preds = model_mv(X_b.to(device)).cpu().numpy()\n",
    "        all_preds_mv.append(preds)\n",
    "\n",
    "y_pred_mv_scaled = np.concatenate(all_preds_mv)\n",
    "\n",
    "# Inverse-transform using only the load column's statistics\n",
    "load_mean = scaler_feat.mean_[0]\n",
    "load_std  = scaler_feat.scale_[0]\n",
    "y_pred_mv_mw = y_pred_mv_scaled * load_std + load_mean\n",
    "y_true_mv_mw = y_test_mv * load_std + load_mean\n",
    "y_true_mv_mw = y_true_mv_mw.reshape(-1, OUTPUT_HOURS)\n",
    "\n",
    "mae_mv  = mean_absolute_error(y_true_mv_mw.flatten(), y_pred_mv_mw.flatten())\n",
    "rmse_mv = np.sqrt(mean_squared_error(y_true_mv_mw.flatten(), y_pred_mv_mw.flatten()))\n",
    "mape_mv = np.mean(np.abs(\n",
    "    (y_true_mv_mw.flatten() - y_pred_mv_mw.flatten()) / y_true_mv_mw.flatten()\n",
    ")) * 100\n",
    "\n",
    "print(f\"\\nMultivariate LSTM (load + weather):\")\n",
    "print(f\"  MAE:  {mae_mv:.4f} MW\")\n",
    "print(f\"  RMSE: {rmse_mv:.4f} MW\")\n",
    "print(f\"  MAPE: {mape_mv:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Rebuild the Guide 02 baseline for Feeder F01\n",
    "# We need to predict 24 steps, so we do iterative 1-step forecasting\n",
    "gb_df = df.copy()\n",
    "gb_df[\"is_weekend\"] = (gb_df[\"day_of_week\"] >= 5).astype(int)\n",
    "gb_df[\"load_lag_24h\"]  = gb_df[\"load_mw\"].shift(24)\n",
    "gb_df[\"load_lag_168h\"] = gb_df[\"load_mw\"].shift(168)\n",
    "gb_df[\"load_rolling_24h\"] = gb_df[\"load_mw\"].rolling(24).mean()\n",
    "gb_df = gb_df.dropna()\n",
    "\n",
    "feature_cols = [\"hour\", \"day_of_week\", \"month\", \"is_weekend\",\n",
    "                \"temperature\", \"humidity\", \"wind_speed\",\n",
    "                \"load_lag_24h\", \"load_lag_168h\", \"load_rolling_24h\"]\n",
    "\n",
    "gb_split = int(len(gb_df) * 0.85)\n",
    "gb_train = gb_df.iloc[:gb_split]\n",
    "gb_test  = gb_df.iloc[gb_split:]\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=300, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(gb_train[feature_cols], gb_train[\"load_mw\"])\n",
    "\n",
    "# 1-step GB prediction on the test set\n",
    "gb_pred = gb_model.predict(gb_test[feature_cols])\n",
    "gb_mae = mean_absolute_error(gb_test[\"load_mw\"], gb_pred)\n",
    "gb_rmse = np.sqrt(mean_squared_error(gb_test[\"load_mw\"], gb_pred))\n",
    "gb_mape = np.mean(np.abs(\n",
    "    (gb_test[\"load_mw\"].values - gb_pred) / gb_test[\"load_mw\"].values\n",
    ")) * 100\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"Model Comparison (Test Set: 2024)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Model':8} {'RMSE':>8} {'MAPE':>8}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'GB (1-step, Guide 02)':8.4f} {gb_rmse:>8.4f} {gb_mape:>7.2f}%\")\n",
    "print(f\"{'LSTM (univariate, 24-step)':8.4f} {rmse:>8.4f} {mape:>7.2f}%\")\n",
    "print(f\"{'LSTM + weather (24-step)':8.4f} {rmse_mv:>8.4f} {mape_mv:>7.2f}%\")\n",
    "print(\"=\" * 55)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visual comparison: one week of forecasts\n",
    "week_idx = slice(500, 668)  # 168 hours = 1 week\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Actual load\n",
    "week_actual = gb_test.iloc[week_idx]\n",
    "ax.plot(week_actual[\"timestamp\"].values,\n",
    "        week_actual[\"load_mw\"].values,\n",
    "        label=\"Actual\", linewidth=2, color=\"#1a202c\")\n",
    "\n",
    "# GB predictions\n",
    "ax.plot(week_actual[\"timestamp\"].values,\n",
    "        gb_pred[week_idx],\n",
    "        label=\"Gradient Boosting (Guide 02)\", linewidth=1.5,\n",
    "        linestyle=\"--\", color=\"#D69E2E\")\n",
    "\n",
    "# LSTM + weather: use the 1-hour-ahead prediction from each sequence\n",
    "lstm_week_pred = y_pred_mv_mw[500:668, 0]  # first step of each forecast\n",
    "ax.plot(week_actual[\"timestamp\"].values[:len(lstm_week_pred)],\n",
    "        lstm_week_pred,\n",
    "        label=\"LSTM + Weather\", linewidth=1.5,\n",
    "        linestyle=\"-.\", color=\"#5FCCDB\")\n",
    "\n",
    "ax.set_title(\"One Week Comparison: Actual vs. GB vs. LSTM+Weather\")\n",
    "ax.set_ylabel(\"Load (MW)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the trained model weights\n",
    "torch.save(model.state_dict(), \"load_forecaster.pt\")\n",
    "\n",
    "# Load them back into a new model instance\n",
    "model = LoadForecaster()\n",
    "model.load_state_dict(torch.load(\"load_forecaster.pt\"))\n",
    "model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}