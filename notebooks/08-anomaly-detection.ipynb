{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "name": "08-anomaly-detection.ipynb"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection & Grid State Estimation\n",
    "\n",
    "From the [Sisyphean Gridworks ML Playground](https://sgridworks.com/ml-playground/guides/08-anomaly-detection.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Clone the repository and install dependencies. Run this cell first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone https://github.com/SGridworks/Dynamic-Network-Model.git 2>/dev/null || echo 'Already cloned'\n",
    "%cd Dynamic-Network-Model\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm pyarrow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore AMI Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from demo_data.load_demo_data import load_customer_interval_data\n",
    "\n",
    "# Load AMI data\n",
    "ami = load_customer_interval_data()\n",
    "\n",
    "print(f\"AMI records: {len(ami):,}\")\n",
    "print(f\"Columns: {list(ami.columns)}\")\n",
    "print(f\"Customers: {ami['customer_id'].nunique()}\")\n",
    "print(ami.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on Voltage Readings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pick one customer to study in detail\n",
    "meter = ami[ami[\"customer_id\"] == ami[\"customer_id\"].unique()[0]].copy()\n",
    "meter[\"timestamp\"] = pd.to_datetime(meter[\"timestamp\"])\n",
    "meter = meter.sort_values(\"timestamp\")\n",
    "\n",
    "# Plot voltage over a month\n",
    "one_month = meter[(meter[\"timestamp\"] >= \"2024-06-01\") &\n",
    "                  (meter[\"timestamp\"] \"2024-07-01\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.plot(one_month[\"timestamp\"], one_month[\"voltage_v\"], linewidth=0.5, color=\"#2D6A7A\")\n",
    "ax.axhline(y=126, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"ANSI upper (126V)\")\n",
    "ax.axhline(y=114, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"ANSI lower (114V)\")\n",
    "ax.set_title(\"AMI Voltage Readings \u2014 June 2024\")\n",
    "ax.set_ylabel(\"Voltage (V)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer Features for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create features from voltage readings across all customers\n",
    "ami[\"timestamp\"] = pd.to_datetime(ami[\"timestamp\"])\n",
    "ami[\"hour\"] = ami[\"timestamp\"].dt.hour\n",
    "\n",
    "# Aggregate to hourly statistics per customer\n",
    "hourly = ami.groupby([\"customer_id\", ami[\"timestamp\"].dt.floor(\"h\")]).agg(\n",
    "    voltage_mean=(\"voltage_v\", \"mean\"),\n",
    "    voltage_std=(\"voltage_v\", \"std\"),\n",
    "    voltage_min=(\"voltage_v\", \"min\"),\n",
    "    voltage_max=(\"voltage_v\", \"max\"),\n",
    "    energy_kwh=(\"energy_kwh\", \"sum\"),\n",
    ").reset_index()\n",
    "\n",
    "# Add voltage range (spread) as a feature\n",
    "hourly[\"voltage_range\"] = hourly[\"voltage_max\"] - hourly[\"voltage_min\"]\n",
    "\n",
    "# Fill any NaN values\n",
    "hourly = hourly.fillna(0)\n",
    "\n",
    "print(f\"Hourly feature rows: {len(hourly):,}\")\n",
    "print(hourly.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select features for anomaly detection\n",
    "feature_cols = [\"voltage_mean\", \"voltage_std\", \"voltage_range\",\n",
    "                \"voltage_min\", \"voltage_max\", \"energy_kwh\"]\n",
    "\n",
    "X = hourly[feature_cols]\n",
    "\n",
    "# Standardize features (important for distance-based methods)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Isolation Forest\n",
    "# contamination = expected % of anomalies (start with 1%)\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "iso_forest.fit(X_scaled)\n",
    "print(\"Isolation Forest training complete.\")\n",
    "\n",
    "# Predict: -1 = anomaly, 1 = normal\n",
    "hourly[\"anomaly\"] = iso_forest.predict(X_scaled)\n",
    "hourly[\"anomaly_score\"] = iso_forest.decision_function(X_scaled)\n",
    "\n",
    "n_anomalies = (hourly[\"anomaly\"] == -1).sum()\n",
    "print(f\"Anomalies detected: {n_anomalies} ({n_anomalies/len(hourly)*100:.2f}%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot anomalies on the voltage timeline\n",
    "anomalies = hourly[hourly[\"anomaly\"] == -1]\n",
    "normal    = hourly[hourly[\"anomaly\"] == 1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.scatter(normal[\"voltage_mean\"], normal[\"voltage_std\"],\n",
    "           c=\"#5FCCDB\", s=5, alpha=0.3, label=\"Normal\")\n",
    "ax.scatter(anomalies[\"voltage_mean\"], anomalies[\"voltage_std\"],\n",
    "           c=\"red\", s=30, marker=\"x\", label=\"Anomaly\")\n",
    "ax.set_xlabel(\"Mean Voltage (V)\")\n",
    "ax.set_ylabel(\"Voltage Std Dev\")\n",
    "ax.set_title(\"Isolation Forest: Anomaly Detection in AMI Voltage Data\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Autoencoder\n",
    "\n",
    "An autoencoder is a neural network that learns to compress data into a small representation and then reconstruct it. If the network is trained on normal data, it will reconstruct normal patterns well but struggle with anomalies\u2014producing high reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "class VoltageAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # Encoder: compress from input_dim down to 3\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 3),  # bottleneck layer\n",
    "        )\n",
    "        # Decoder: reconstruct from 3 back to input_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Create the model\n",
    "input_dim = len(feature_cols)\n",
    "model = VoltageAutoencoder(input_dim)\n",
    "print(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Autoencoder on Normal Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use only normal data for training (filter out Isolation Forest anomalies)\n",
    "normal_data = hourly[hourly[\"anomaly\"] == 1][feature_cols]\n",
    "normal_scaled = scaler.fit_transform(normal_data)\n",
    "\n",
    "# Split into train (80%) and validation (20%)\n",
    "split = int(len(normal_scaled) * 0.8)\n",
    "train_data = torch.FloatTensor(normal_scaled[:split])\n",
    "val_data   = torch.FloatTensor(normal_scaled[split:])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(TensorDataset(train_data, train_data),\n",
    "                          batch_size=64, shuffle=True)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train for 50 epochs\n",
    "losses = []\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:>3}/50  Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, color=\"#5FCCDB\")\n",
    "plt.title(\"Autoencoder Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Anomalies by Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run ALL data through the autoencoder (normal + anomalous)\n",
    "model.eval()\n",
    "all_scaled = scaler.transform(hourly[feature_cols])\n",
    "all_tensor = torch.FloatTensor(all_scaled)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(all_tensor)\n",
    "    recon_error = torch.mean((all_tensor - reconstructed) ** 2, dim=1)\n",
    "\n",
    "hourly[\"recon_error\"] = recon_error.numpy()\n",
    "\n",
    "# Set threshold at 99th percentile of reconstruction error\n",
    "threshold = hourly[\"recon_error\"].quantile(0.99)\n",
    "hourly[\"ae_anomaly\"] = (hourly[\"recon_error\"] > threshold).astype(int)\n",
    "\n",
    "print(f\"Reconstruction error threshold: {threshold:.4f}\")\n",
    "print(f\"Autoencoder anomalies: {hourly['ae_anomaly'].sum()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Both Methods"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare Isolation Forest vs Autoencoder detections\n",
    "hourly[\"iso_anomaly\"] = (hourly[\"anomaly\"] == -1).astype(int)\n",
    "\n",
    "both = (hourly[\"iso_anomaly\"] & hourly[\"ae_anomaly\"]).sum()\n",
    "iso_only = (hourly[\"iso_anomaly\"] & ~hourly[\"ae_anomaly\"]).sum()\n",
    "ae_only  = (~hourly[\"iso_anomaly\"] & hourly[\"ae_anomaly\"]).sum()\n",
    "\n",
    "print(f\"Flagged by both methods:       {both}\")\n",
    "print(f\"Isolation Forest only:         {iso_only}\")\n",
    "print(f\"Autoencoder only:              {ae_only}\")\n",
    "\n",
    "# Reconstruction error distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(hourly[\"recon_error\"], bins=100, color=\"#5FCCDB\", edgecolor=\"white\")\n",
    "ax.axvline(x=threshold, color=\"red\", linestyle=\"--\",\n",
    "           label=f\"Threshold ({threshold:.4f})\")\n",
    "ax.set_xlabel(\"Reconstruction Error\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Autoencoder Reconstruction Error Distribution\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Look at the top anomalies detected by both methods\n",
    "high_confidence = hourly[(hourly[\"iso_anomaly\"] == 1) & (hourly[\"ae_anomaly\"] == 1)]\n",
    "high_confidence = high_confidence.sort_values(\"recon_error\", ascending=False)\n",
    "\n",
    "print(\"Top 10 highest-confidence anomalies:\\n\")\n",
    "print(high_confidence[[\"customer_id\", \"timestamp\", \"voltage_mean\",\n",
    "      \"voltage_std\", \"voltage_range\", \"recon_error\"]].head(10).to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Built and Next Steps"
   ]
  }
 ]
}