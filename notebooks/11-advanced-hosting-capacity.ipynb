{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "name": "11-advanced-hosting-capacity.ipynb"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Accelerated Hosting Capacity Screening\n",
    "\n",
    "From the [Sisyphean Gridworks ML Playground](https://sgridworks.com/ml-playground/guides/11-advanced-hosting-capacity.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Clone the repository and install dependencies. Run this cell first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone https://github.com/SGridworks/Dynamic-Network-Model.git 2>/dev/null || echo 'Already cloned'\n",
    "%cd Dynamic-Network-Model\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm pyarrow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SP&L Data and Compute Hosting Capacity\n",
    "\n",
    "We load the SP&L network, transformer, solar, and load profile data using the data loader API. Then we compute hosting capacity per transformer as a simplified estimate: rated kVA minus existing solar capacity minus peak load. These computed values become the labels for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import time\n",
    "\n",
    "from demo_data.load_demo_data import (\n",
    "    load_network_nodes,\n",
    "    load_network_edges,\n",
    "    load_transformers,\n",
    "    load_solar_installations,\n",
    "    load_load_profiles,\n",
    ")\n",
    "\n",
    "# --- Load all datasets via the SP&L data loader ---\n",
    "nodes = load_network_nodes()         # ~44k nodes with lat/lon\n",
    "edges = load_network_edges()         # ~44k edges with impedance, rated amps\n",
    "transformers = load_transformers()   # ~21k transformers with kva_rating\n",
    "solar = load_solar_installations()   # ~17k solar installs with capacity_kw\n",
    "load_profiles = load_load_profiles() # 15-min feeder load profiles\n",
    "\n",
    "print(f\"Network nodes:        {len(nodes):,}\")\n",
    "print(f\"Network edges:        {len(edges):,}\")\n",
    "print(f\"Transformers:         {len(transformers):,}\")\n",
    "print(f\"Solar installations:  {len(solar):,}\")\n",
    "print(f\"Load profile rows:    {len(load_profiles):,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Transformer and Network Features\n",
    "\n",
    "A good surrogate model needs features that capture the physical factors driving hosting capacity: how far the transformer is from the substation, how much load is nearby, how stiff the local network is. We engineer six groups of features from the SP&L data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Compute hosting capacity per transformer ---\n",
    "# Existing solar capacity aggregated to each transformer\n",
    "solar_by_xfmr = solar.groupby(\"transformer_id\")[\"capacity_kw\"].sum()\n",
    "solar_by_xfmr.name = \"existing_solar_kw\"\n",
    "\n",
    "# Peak load per feeder from load profiles (MW -> kW)\n",
    "feeder_peak = load_profiles.groupby(\"feeder_id\")[\"load_mw\"].max().reset_index()\n",
    "feeder_peak.columns = [\"feeder_id\", \"peak_load_mw\"]\n",
    "\n",
    "# Count transformers per feeder to allocate feeder load\n",
    "xfmr_counts = transformers.groupby(\"feeder_id\").size().reset_index(name=\"n_xfmrs\")\n",
    "feeder_peak = feeder_peak.merge(xfmr_counts, on=\"feeder_id\")\n",
    "feeder_peak[\"load_per_xfmr_kw\"] = (\n",
    "    feeder_peak[\"peak_load_mw\"] * 1000 / feeder_peak[\"n_xfmrs\"]\n",
    ")\n",
    "\n",
    "# Build the HCA table: capacity minus solar minus load\n",
    "hca = transformers[[\"feeder_id\", \"substation_id\",\n",
    "                     \"kva_rating\", \"age_years\",\n",
    "                     \"latitude\", \"longitude\"]].copy()\n",
    "hca = hca.merge(solar_by_xfmr, left_index=True,\n",
    "                right_index=True, how=\"left\")\n",
    "hca[\"existing_solar_kw\"] = hca[\"existing_solar_kw\"].fillna(0)\n",
    "hca = hca.merge(\n",
    "    feeder_peak[[\"feeder_id\", \"load_per_xfmr_kw\"]],\n",
    "    on=\"feeder_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Hosting capacity = rated capacity - existing solar - allocated load\n",
    "hca[\"hosting_capacity_kw\"] = (\n",
    "    hca[\"kva_rating\"] - hca[\"existing_solar_kw\"] - hca[\"load_per_xfmr_kw\"]\n",
    ").clip(lower=0)\n",
    "\n",
    "print(f\"\\nHosting capacity computed for {len(hca):,} transformers\")\n",
    "print(f\"Across {hca['feeder_id'].nunique()} feeders\")\n",
    "print(f\"\\nHosting capacity summary (kW):\")\n",
    "print(hca[\"hosting_capacity_kw\"].describe().round(1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Labels and Split\n",
    "\n",
    "The target variable is hosting_capacity_kw\u2014the estimated remaining capacity at each transformer after accounting for existing solar and allocated peak load. This is a regression problem: we predict a continuous value, not a category."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Feature Group 1: Distance from substation ---\n",
    "# Compute distance from each transformer to its substation node\n",
    "sub_nodes = nodes[nodes[\"node_type\"] == \"substation_bus\"]\n",
    "sub_coords = sub_nodes[[\"latitude\", \"longitude\"]].reset_index()\n",
    "sub_coords.columns = [\"substation_id\", \"sub_lat\", \"sub_lon\"]\n",
    "\n",
    "hca = hca.merge(sub_coords, on=\"substation_id\", how=\"left\")\n",
    "\n",
    "# Approximate distance in km using lat/lon\n",
    "hca[\"dist_from_sub_km\"] = np.sqrt(\n",
    "    ((hca[\"latitude\"] - hca[\"sub_lat\"]) * 111) ** 2 +\n",
    "    ((hca[\"longitude\"] - hca[\"sub_lon\"]) * 85) ** 2\n",
    ")\n",
    "\n",
    "# --- Feature Group 2: Transformer characteristics ---\n",
    "# kva_rating and age_years are already in the hca table from Step 1\n",
    "\n",
    "# --- Feature Group 3: Conductor impedance from network edges ---\n",
    "# Aggregate edge data per feeder for impedance and ampacity features\n",
    "feeder_edges = edges.groupby(\"feeder_id\").agg({\n",
    "    \"rated_amps\":              \"min\",    # bottleneck conductor\n",
    "    \"impedance_r_ohm_per_mile\": \"mean\",  # average resistance\n",
    "    \"length_miles\":            \"sum\",    # total line length\n",
    "}).reset_index()\n",
    "feeder_edges.columns = [\"feeder_id\", \"min_rated_amps\",\n",
    "                         \"avg_r_ohm_per_mi\", \"total_line_mi\"]\n",
    "\n",
    "print(\"Feature groups created:\")\n",
    "print(f\"  Transformers with distance: {len(hca):,}\")\n",
    "print(f\"  Feeder edge features: {len(feeder_edges):,} feeders\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a LightGBM Regression Model\n",
    "\n",
    "LightGBM uses gradient-boosted decision trees with histogram-based splitting for speed. For hosting capacity prediction, regression is the right objective: we want to predict a continuous kW value, not a category."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Feature Group 4: Load density ---\n",
    "# Peak load per feeder from load profiles (already computed in Step 1)\n",
    "# Count transformers per feeder for load density\n",
    "feeder_stats = feeder_peak[[\"feeder_id\", \"peak_load_mw\", \"n_xfmrs\"]].copy()\n",
    "feeder_stats[\"peak_load_kw\"] = feeder_stats[\"peak_load_mw\"] * 1000\n",
    "feeder_stats[\"load_density_kw_per_xfmr\"] = (\n",
    "    feeder_stats[\"peak_load_kw\"] / feeder_stats[\"n_xfmrs\"]\n",
    ")\n",
    "\n",
    "# --- Feature Group 5: Existing DER penetration ---\n",
    "feeder_solar = solar.groupby(\"feeder_id\")[\"capacity_kw\"].sum().reset_index()\n",
    "feeder_solar.columns = [\"feeder_id\", \"existing_solar_total_kw\"]\n",
    "feeder_stats = feeder_stats.merge(feeder_solar, on=\"feeder_id\", how=\"left\")\n",
    "feeder_stats[\"existing_solar_total_kw\"] = feeder_stats[\"existing_solar_total_kw\"].fillna(0)\n",
    "feeder_stats[\"pv_penetration_pct\"] = (\n",
    "    feeder_stats[\"existing_solar_total_kw\"] / feeder_stats[\"peak_load_kw\"] * 100\n",
    ")\n",
    "\n",
    "# --- Feature Group 6: Merge everything into one table ---\n",
    "df = hca.copy()\n",
    "df = df.merge(feeder_edges, on=\"feeder_id\", how=\"left\")\n",
    "df = df.merge(\n",
    "    feeder_stats[[\"feeder_id\", \"peak_load_kw\", \"n_xfmrs\",\n",
    "                  \"load_density_kw_per_xfmr\",\n",
    "                  \"existing_solar_total_kw\", \"pv_penetration_pct\"]],\n",
    "    on=\"feeder_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Fill missing values with 0\n",
    "# NOTE: LightGBM handles NaN natively by routing missing values to the\n",
    "# optimal split direction. Using fillna(0) is a deliberate choice here\n",
    "# because a transformer with no edge data genuinely has zero local\n",
    "# impedance. However, be careful with this pattern: if 0 has a\n",
    "# meaningful non-missing interpretation for a feature (e.g.,\n",
    "# age_years=0 could imply a brand-new transformer rather than missing\n",
    "# data), consider using a sentinel value like -1 or keeping NaN.\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(df):,} rows x {len(df.columns)} columns\")\n",
    "print(f\"Features available: {len(df.columns) - 4}\")  # subtract IDs and target"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with R\u00b2, MAE, and Scatter Plot\n",
    "\n",
    "A good surrogate model should show predictions tightly clustered around the 45-degree line (predicted = actual). We also check feature importance to validate the model learned physically meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define features and target\n",
    "feature_cols = [\n",
    "    \"dist_from_sub_km\",\n",
    "    \"kva_rating\", \"age_years\",\n",
    "    \"min_rated_amps\", \"avg_r_ohm_per_mi\", \"total_line_mi\",\n",
    "    \"peak_load_kw\", \"n_xfmrs\", \"load_density_kw_per_xfmr\",\n",
    "    \"existing_solar_kw\",\n",
    "    \"existing_solar_total_kw\", \"pv_penetration_pct\",\n",
    "    \"load_per_xfmr_kw\",\n",
    "]\n",
    "target_col = \"hosting_capacity_kw\"\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "# 80/20 train/test split stratified by feeder to ensure all feeders\n",
    "# are represented in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42,\n",
    "    stratify=df[\"feeder_id\"]\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} transformers\")\n",
    "print(f\"Test set:     {len(X_test):,} transformers\")\n",
    "print(f\"\\nTarget distribution (training):\")\n",
    "print(f\"  Mean: {y_train.mean():.0f} kW\")\n",
    "print(f\"  Std:  {y_train.std():.0f} kW\")\n",
    "print(f\"  Min:  {y_train.min():.0f} kW\")\n",
    "print(f\"  Max:  {y_train.max():.0f} kW\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis\n",
    "\n",
    "A surrogate model lets you explore \"what-if\" scenarios instantly. We vary temperature (which affects conductor ratings), load growth, and inverter power factor to see how hosting capacity shifts across the network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train,\n",
    "                          feature_name=feature_cols)\n",
    "test_data = lgb.Dataset(X_test, label=y_test,\n",
    "                         reference=train_data)\n",
    "\n",
    "# Model parameters\n",
    "params = {\n",
    "    \"objective\":       \"regression\",\n",
    "    \"metric\":          \"mae\",\n",
    "    \"boosting_type\":   \"gbdt\",\n",
    "    \"num_leaves\":      31,\n",
    "    \"learning_rate\":   0.05,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\":    5,\n",
    "    \"min_child_samples\": 10,\n",
    "    \"verbose\":         -1,\n",
    "    \"seed\":            42,\n",
    "}\n",
    "\n",
    "# Train with early stopping\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=50),\n",
    "    lgb.log_evaluation(period=100),\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_data, test_data],\n",
    "    valid_names=[\"train\", \"test\"],\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {model.best_iteration}\")\n",
    "# LightGBM stores the metric under 'l1' internally even when you pass metric='mae'\n",
    "best_scores = model.best_score.get(\"test\", {})\n",
    "best_mae = best_scores.get(\"mae\", best_scores.get(\"l1\", None))\n",
    "print(f\"Best test MAE:  {best_mae:.1f} kW\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Hosting Capacity with Quantile Regression\n",
    "\n",
    "Point estimates are useful, but planners need ranges. \"The hosting capacity is between 80 and 180 kW with 80% confidence\" is more actionable than \"the hosting capacity is 130 kW.\" LightGBM supports quantile regression natively."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"R-squared: {r2:.3f}\")\n",
    "print(f\"MAE:       {mae:.1f} kW\")\n",
    "\n",
    "# Scatter plot: predicted vs actual\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: predicted vs actual\n",
    "ax1.scatter(y_test, y_pred, alpha=0.6, c=\"#5FCCDB\", edgecolor=\"#2D6A7A\", s=40)\n",
    "ax1.plot([0, 500], [0, 500], \"r--\", linewidth=1.5, label=\"Perfect prediction\")\n",
    "ax1.set_xlabel(\"Actual Hosting Capacity (kW)\")\n",
    "ax1.set_ylabel(\"Predicted Hosting Capacity (kW)\")\n",
    "ax1.set_title(f\"Predicted vs Actual (R\\u00b2={r2:.3f}, MAE={mae:.0f} kW)\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: feature importance\n",
    "importance = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": model.feature_importance(importance_type=\"gain\")\n",
    "}).sort_values(\"importance\", ascending=True)\n",
    "\n",
    "ax2.barh(importance[\"feature\"], importance[\"importance\"], color=\"#2D6A7A\")\n",
    "ax2.set_xlabel(\"Feature Importance (Gain)\")\n",
    "ax2.set_title(\"LightGBM Feature Importance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Hosting Capacity Spatially\n",
    "\n",
    "Utility planners think spatially. A hosting capacity \"heat map\" shows at a glance where the grid can absorb more solar and where it cannot. We use the latitude and longitude from the SP&L transformer data to plot predicted hosting capacity geographically."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Scenario analysis: vary one feature at a time\n",
    "baseline = X_test.copy()\n",
    "scenarios = {}\n",
    "\n",
    "# Scenario 1: Summer peak (reduce ampacity by 15% due to temperature)\n",
    "summer_peak = baseline.copy()\n",
    "summer_peak[\"min_rated_amps\"] *= 0.85\n",
    "scenarios[\"Summer derating (-15% ampacity)\"] = summer_peak\n",
    "\n",
    "# Scenario 2: 20% load growth\n",
    "load_growth = baseline.copy()\n",
    "load_growth[\"peak_load_kw\"] *= 1.20\n",
    "load_growth[\"load_density_kw_per_xfmr\"] *= 1.20\n",
    "load_growth[\"load_per_xfmr_kw\"] *= 1.20\n",
    "scenarios[\"20% load growth\"] = load_growth\n",
    "\n",
    "# Scenario 3: High existing DER (double current PV penetration)\n",
    "high_der = baseline.copy()\n",
    "high_der[\"existing_solar_kw\"] *= 2.0\n",
    "high_der[\"existing_solar_total_kw\"] *= 2.0\n",
    "high_der[\"pv_penetration_pct\"] *= 2.0\n",
    "scenarios[\"2x existing PV penetration\"] = high_der\n",
    "\n",
    "# Predict hosting capacity for each scenario\n",
    "results = {\"Baseline\": model.predict(baseline, num_iteration=model.best_iteration)}\n",
    "for name, scenario_X in scenarios.items():\n",
    "    results[name] = model.predict(scenario_X, num_iteration=model.best_iteration)\n",
    "\n",
    "# Compare median hosting capacity across scenarios\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "medians = {k: np.median(v) for k, v in results.items()}\n",
    "colors = [\"#5FCCDB\", \"#E53E3E\", \"#D69E2E\", \"#9F7AEA\"]\n",
    "ax.bar(medians.keys(), medians.values(), color=colors)\n",
    "ax.set_ylabel(\"Median Hosting Capacity (kW)\")\n",
    "ax.set_title(\"Sensitivity Analysis: Impact on Hosting Capacity\")\n",
    "plt.xticks(rotation=20, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print percentage changes\n",
    "baseline_median = medians[\"Baseline\"]\n",
    "print(\"Scenario impact vs baseline:\")\n",
    "for name, med in medians.items():\n",
    "    pct = (med - baseline_median) / baseline_median * 100\n",
    "    print(f\"  {name}: {med:.0f} kW ({pct:+.1f}%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: ML Screening vs Full Recomputation\n",
    "\n",
    "The whole point of a surrogate model is speed. Let us quantify how much faster ML screening is compared to recomputing hosting capacity from scratch for every transformer. The ML surrogate skips the per-transformer aggregation of solar, load allocation, and capacity arithmetic. For the specific task of screening hosting capacity values, the speed advantage is enormous."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train quantile models for 10th, 50th, and 90th percentiles\n",
    "quantile_models = {}\n",
    "\n",
    "for alpha in [0.10, 0.50, 0.90]:\n",
    "    q_params = params.copy()\n",
    "    q_params[\"objective\"] = \"quantile\"\n",
    "    q_params[\"alpha\"] = alpha\n",
    "    q_params[\"metric\"] = \"quantile\"\n",
    "\n",
    "    q_model = lgb.train(\n",
    "        q_params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[test_data],\n",
    "        valid_names=[\"test\"],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)],\n",
    "    )\n",
    "    quantile_models[alpha] = q_model\n",
    "    print(f\"  Quantile {alpha:.0%} model trained ({q_model.best_iteration} rounds)\")\n",
    "\n",
    "# Predict quantile ranges\n",
    "q10 = quantile_models[0.10].predict(X_test)\n",
    "q50 = quantile_models[0.50].predict(X_test)\n",
    "q90 = quantile_models[0.90].predict(X_test)\n",
    "\n",
    "# Plot quantile predictions for a subset of transformers\n",
    "n_show = 25\n",
    "idx = np.argsort(y_test.values)[:n_show]  # sort by actual HC\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x_pos = np.arange(n_show)\n",
    "\n",
    "# 80% prediction interval (10th to 90th percentile)\n",
    "ax.fill_between(x_pos, q10[idx], q90[idx], alpha=0.3,\n",
    "                 color=\"#5FCCDB\", label=\"80% prediction interval\")\n",
    "ax.plot(x_pos, q50[idx], \"o-\", color=\"#2D6A7A\",\n",
    "        linewidth=2, markersize=5, label=\"Median prediction\")\n",
    "ax.plot(x_pos, y_test.values[idx], \"s\", color=\"#E53E3E\",\n",
    "        markersize=6, label=\"Actual (computed)\")\n",
    "\n",
    "ax.set_xlabel(\"Transformer (sorted by actual hosting capacity)\")\n",
    "ax.set_ylabel(\"Hosting Capacity (kW)\")\n",
    "ax.set_title(\"Probabilistic Hosting Capacity: 80% Prediction Interval\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Coverage: what fraction of actuals fall within the 80% interval?\n",
    "coverage = np.mean((y_test.values >= q10) & (y_test.values mean(q90 - q10)\n",
    "print(f\"\\n80% interval coverage: {coverage:.1%} (target: 80%)\")\n",
    "print(f\"Average interval width: {avg_width:.0f} kW\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Built and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predict hosting capacity for ALL transformers (not just test set)\n",
    "y_all_pred = model.predict(X, num_iteration=model.best_iteration)\n",
    "\n",
    "# Build map dataframe with predictions and coordinates\n",
    "map_df = df[[\"feeder_id\", \"latitude\", \"longitude\"]].copy()\n",
    "map_df[\"hc_predicted_kw\"] = y_all_pred\n",
    "\n",
    "# Spatial hosting capacity map\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    map_df[\"longitude\"], map_df[\"latitude\"],\n",
    "    c=map_df[\"hc_predicted_kw\"],\n",
    "    cmap=\"RdYlGn\",\n",
    "    s=20,\n",
    "    edgecolor=\"#333\",\n",
    "    linewidth=0.3,\n",
    "    alpha=0.7,\n",
    "    vmin=0,\n",
    "    vmax=500,\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "cbar.set_label(\"Predicted Hosting Capacity (kW)\", fontsize=12)\n",
    "\n",
    "# Label substations from the nodes table\n",
    "sub_locs = nodes[nodes[\"node_type\"] == \"substation_bus\"]\n",
    "ax.scatter(sub_locs[\"longitude\"], sub_locs[\"latitude\"],\n",
    "           marker=\"^\", s=200, c=\"black\", zorder=5,\n",
    "           label=\"Substation\")\n",
    "\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_title(\"SP&L Service Territory: ML-Predicted Hosting Capacity\")\n",
    "ax.legend(loc=\"upper left\", fontsize=11)\n",
    "ax.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary by feeder\n",
    "feeder_summary = map_df.groupby(\"feeder_id\")[\"hc_predicted_kw\"].agg(\n",
    "    [\"mean\", \"min\", \"max\"]\n",
    ").round(0)\n",
    "feeder_summary.columns = [\"Mean HC (kW)\", \"Min HC (kW)\", \"Max HC (kW)\"]\n",
    "print(\"\\nHosting capacity by feeder (first 10):\")\n",
    "print(feeder_summary.head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Time the ML prediction for all transformers\n",
    "start_ml = time.perf_counter()\n",
    "for _ in range(100):  # run 100 times for stable measurement\n",
    "    ml_pred = model.predict(X, num_iteration=model.best_iteration)\n",
    "end_ml = time.perf_counter()\n",
    "ml_time_per_run = (end_ml - start_ml) / 100\n",
    "\n",
    "# Time the pandas-based full computation for comparison\n",
    "start_pd = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    _solar_agg = solar.groupby(\"transformer_id\")[\"capacity_kw\"].sum()\n",
    "    _peak = load_profiles.groupby(\"feeder_id\")[\"load_mw\"].max()\n",
    "end_pd = time.perf_counter()\n",
    "pd_time_per_run = (end_pd - start_pd) / 10\n",
    "\n",
    "print(\"=== Speed Comparison ===\")\n",
    "print(f\"ML surrogate (all {len(X):,} transformers): {ml_time_per_run*1000:.1f} ms\")\n",
    "print(f\"Pandas recomputation:  {pd_time_per_run*1000:.0f} ms\")\n",
    "print(f\"Speedup: {pd_time_per_run / ml_time_per_run:,.0f}x\")\n",
    "\n",
    "# Scaling comparison (what-if for larger utilities)\n",
    "xfmr_counts = [1000, 5000, 21000, 50000, 100000]\n",
    "scale = len(X)\n",
    "print(f\"\\n{'Transformers':>14}  {'Full Recompute':>15}  {'ML Model':>12}  {'Speedup':>10}\")\n",
    "print(\"-\" * 58)\n",
    "for n in xfmr_counts:\n",
    "    pd_t = pd_time_per_run * (n / scale)\n",
    "    ml_t = ml_time_per_run * (n / scale)\n",
    "    if pd_t >= 1:\n",
    "        pd_str = f\"{pd_t:.1f} sec\"\n",
    "    else:\n",
    "        pd_str = f\"{pd_t*1000:.0f} ms\"\n",
    "    print(f\"{n:>14,}  {pd_str:>15}  {ml_t*1000:>9.1f} ms  {pd_t/ml_t:>10,.0f}x\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the trained LightGBM model\n",
    "model.save_model(\"hosting_capacity_surrogate.lgb\")\n",
    "\n",
    "# Load it back\n",
    "model = lgb.Booster(model_file=\"hosting_capacity_surrogate.lgb\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}