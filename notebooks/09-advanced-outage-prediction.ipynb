{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "name": "09-advanced-outage-prediction.ipynb"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Outage Cause Prediction with XGBoost & SHAP\n",
    "\n",
    "From the [Sisyphean Gridworks ML Playground](https://sgridworks.com/ml-playground/guides/09-advanced-outage-prediction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Clone the repository and install dependencies. Run this cell first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone https://github.com/SGridworks/Dynamic-Network-Model.git 2>/dev/null || echo 'Already cloned'\n",
    "%cd Dynamic-Network-Model\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm pyarrow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Merge All Data Sources\n",
    "\n",
    "Unlike Guide 01 where we used only weather data, here we merge weather and asset data to give the model a richer picture of outage drivers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from demo_data.load_demo_data import (\n",
    "    load_outage_history, load_weather_data,\n",
    "    load_transformers, load_network_edges\n",
    ")\n",
    "\n",
    "# Load all datasets\n",
    "outages = load_outage_history()\n",
    "weather = load_weather_data()\n",
    "transformers = load_transformers()\n",
    "edges = load_network_edges()\n",
    "\n",
    "print(f\"Outages: {len(outages):,} events\")\n",
    "print(f\"Cause codes: {outages['cause_code'].unique()}\")\n",
    "print(f\"\\nCause code distribution:\")\n",
    "print(outages[\"cause_code\"].value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Enriched Feature Set\n",
    "\n",
    "We combine daily weather summaries with asset condition data for each outage's feeder. This gives the model both environmental and infrastructure context. Note that we keep all cause codes including \u201cunknown\u201d\u2014in real utility data, a significant portion of outages have undetermined causes, and the model should learn to recognize these patterns rather than ignoring them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create daily weather features (same as Guide 01)\n",
    "weather[\"date\"] = weather[\"timestamp\"].dt.date\n",
    "daily_weather = weather.groupby(\"date\").agg({\n",
    "    \"temperature\":    [\"max\", \"min\", \"mean\"],\n",
    "    \"wind_speed\":     [\"max\", \"mean\"],\n",
    "    \"is_storm\":       \"max\",\n",
    "    \"humidity\":       \"mean\",\n",
    "}).reset_index()\n",
    "daily_weather.columns = [\"date\", \"temp_max\", \"temp_min\", \"temp_mean\",\n",
    "                          \"wind_max\", \"wind_mean\", \"is_storm\", \"humidity_mean\"]\n",
    "\n",
    "# Add asset features per feeder\n",
    "feeder_assets = transformers.groupby(\"feeder_id\").agg({\n",
    "    \"age_years\":  \"mean\",\n",
    "    \"kva_rating\": \"sum\",\n",
    "}).reset_index()\n",
    "feeder_assets.columns = [\"feeder_id\", \"avg_asset_age\", \"total_kva\"]\n",
    "\n",
    "# Build the training table: one row per outage event\n",
    "outages[\"date\"] = outages[\"fault_detected\"].dt.date\n",
    "df = outages.merge(daily_weather, on=\"date\", how=\"left\")\n",
    "df = df.merge(feeder_assets, on=\"feeder_id\", how=\"left\")\n",
    "\n",
    "# Add time features\n",
    "df[\"fault_detected\"] = pd.to_datetime(df[\"fault_detected\"])\n",
    "df[\"month\"] = df[\"fault_detected\"].dt.month\n",
    "df[\"hour\"] = df[\"fault_detected\"].dt.hour\n",
    "df[\"day_of_week\"] = df[\"fault_detected\"].dt.dayofweek\n",
    "df[\"is_summer\"] = df[\"month\"].isin([6, 7, 8]).astype(int)\n",
    "df[\"is_storm_season\"] = df[\"month\"].isin([3, 4, 5, 6]).astype(int)\n",
    "\n",
    "# Drop rows with missing weather data (keep all cause codes including unknown)\n",
    "df = df.dropna(subset=[\"temp_max\"])\n",
    "\n",
    "print(f\"Training samples: {len(df):,}\")\n",
    "print(f\"\\nFeatures built per event:\")\n",
    "print(f\"  Weather: 7 | Asset: 2 | Calendar: 5\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Aware Train/Test Split\n",
    "\n",
    "In Guide 01, we used a random split. But in production, your model always predicts the future based on the past. A time-aware split is more honest: train on 2020\u20132023 data, test on 2024\u20132025."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define features and target\n",
    "feature_cols = [\n",
    "    \"temp_max\", \"temp_min\", \"temp_mean\",\n",
    "    \"wind_max\", \"wind_mean\", \"is_storm\", \"humidity_mean\",\n",
    "    \"avg_asset_age\", \"total_kva\",\n",
    "    \"month\", \"hour\", \"day_of_week\", \"is_summer\", \"is_storm_season\"\n",
    "]\n",
    "\n",
    "# Encode cause codes as integers\n",
    "le = LabelEncoder()\n",
    "df[\"cause_label\"] = le.fit_transform(df[\"cause_code\"])\n",
    "cause_names = le.classes_\n",
    "\n",
    "# Time-aware split: train on 2020-2023, test on 2024-2025\n",
    "train_mask = df[\"fault_detected\"].dt.year 2023\n",
    "test_mask  = df[\"fault_detected\"].dt.year >= 2024\n",
    "\n",
    "X_train = df.loc[train_mask, feature_cols]\n",
    "y_train = df.loc[train_mask, \"cause_label\"]\n",
    "X_test  = df.loc[test_mask, feature_cols]\n",
    "y_test  = df.loc[test_mask, \"cause_label\"]\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} events (2020-2023)\")\n",
    "print(f\"Test set:     {len(X_test):,} events (2024-2025)\")\n",
    "print(f\"Classes: {list(cause_names)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost Multi-Class Classifier\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) builds trees sequentially, where each new tree corrects the mistakes of the previous ones. It typically outperforms Random Forest on structured data, especially with class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "class_counts = y_train.value_counts().sort_index()\n",
    "total = len(y_train)\n",
    "n_classes = len(class_counts)\n",
    "class_weights = {i: total / (n_classes * count)\n",
    "                 for i, count in class_counts.items()}\n",
    "\n",
    "# Assign sample weights\n",
    "sample_weights = y_train.map(class_weights)\n",
    "\n",
    "# Train XGBoost\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=n_classes,\n",
    "    random_state=42,\n",
    "    eval_metric=\"mlogloss\",\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    sample_weight=sample_weights,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Multi-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Classification report with cause code names\n",
    "print(classification_report(y_test, y_pred,\n",
    "      target_names=cause_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=cause_names, yticklabels=cause_names, ax=ax)\n",
    "ax.set_xlabel(\"Predicted Cause\")\n",
    "ax.set_ylabel(\"Actual Cause\")\n",
    "ax.set_title(\"Multi-Class Confusion Matrix: Outage Cause Prediction\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Predictions with SHAP\n",
    "\n",
    "Feature importance tells you which features matter globally. SHAP (SHapley Additive exPlanations) goes further: it tells you how much each feature contributed to a specific prediction and in which direction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot: how each feature affects each class\n",
    "fig, axes = plt.subplots(1, len(cause_names), figsize=(20, 6))\n",
    "for i, cause in enumerate(cause_names):\n",
    "    plt.sca(axes[i])\n",
    "    shap.summary_plot(shap_values[i], X_test,\n",
    "                       feature_names=feature_cols,\n",
    "                       show=False, max_display=8)\n",
    "    axes[i].set_title(f\"{cause}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Individual Predictions\n",
    "\n",
    "SHAP's real power is explaining individual events. Pick a specific outage and see exactly why the model predicted its cause."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pick a specific outage event from the test set\n",
    "event_idx = 0\n",
    "event = X_test.iloc[event_idx]\n",
    "actual = cause_names[y_test.iloc[event_idx]]\n",
    "predicted = cause_names[y_pred[event_idx]]\n",
    "\n",
    "print(f\"Event details:\")\n",
    "print(f\"  Actual cause:    {actual}\")\n",
    "print(f\"  Predicted cause: {predicted}\")\n",
    "print(f\"  Wind max:        {event['wind_max']:.1f} mph\")\n",
    "print(f\"  Storm flag:      {event['is_storm']}\")\n",
    "print(f\"  Avg asset age:   {event['avg_asset_age']:.0f} years\")\n",
    "\n",
    "# Waterfall plot for this prediction\n",
    "predicted_class = y_pred[event_idx]\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(\n",
    "        values=shap_values[predicted_class][event_idx],\n",
    "        base_values=explainer.expected_value[predicted_class],\n",
    "        data=event.values,\n",
    "        feature_names=feature_cols\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Against SAIFI Metrics\n",
    "\n",
    "How does your model's predicted outage distribution compare to SP&L's actual reliability metrics? This bridges the gap between ML model accuracy and real utility KPIs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute SAIFI from outage_history (no separate reliability file needed)\n",
    "total_customers = 48_000  # SP&L total customers served\n",
    "annual_saifi = (outages.groupby(outages[\"fault_detected\"].dt.year)[\"affected_customers\"]\n",
    "                .sum() / total_customers)\n",
    "print(\"Annual SAIFI (computed from outage_history):\")\n",
    "print(annual_saifi)\n",
    "\n",
    "# Compare predicted vs actual cause distribution for test years\n",
    "test_outages = df[test_mask]\n",
    "\n",
    "actual_dist = test_outages[\"cause_code\"].value_counts(normalize=True)\n",
    "pred_causes = pd.Series(cause_names[y_pred])\n",
    "pred_dist = pred_causes.value_counts(normalize=True)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Actual %\": (actual_dist * 100).round(1),\n",
    "    \"Predicted %\": (pred_dist * 100).round(1)\n",
    "})\n",
    "print(\"\\nCause distribution comparison (test set):\")\n",
    "print(comparison)\n",
    "\n",
    "# Plot side-by-side\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, comparison[\"Actual %\"], width, label=\"Actual\", color=\"#2D6A7A\")\n",
    "ax.bar(x + width/2, comparison[\"Predicted %\"], width, label=\"Predicted\", color=\"#5FCCDB\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison.index, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Percentage of Outages\")\n",
    "ax.set_title(\"Predicted vs Actual Outage Cause Distribution (2024-2025)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal Cause Analysis\n",
    "\n",
    "Outage causes vary by season. Vegetation peaks in spring/summer during the growing season. Weather outages spike during storm season. Equipment failures may increase in extreme heat. Let's validate the model captures these patterns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predicted causes by month\n",
    "test_outages_with_pred = test_outages.copy()\n",
    "test_outages_with_pred[\"predicted_cause\"] = cause_names[y_pred]\n",
    "\n",
    "monthly_causes = test_outages_with_pred.groupby(\n",
    "    [\"month\", \"predicted_cause\"]\n",
    ").size().unstack(fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "monthly_causes.plot(kind=\"bar\", stacked=True, ax=ax,\n",
    "                    colormap=\"Set2\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Predicted Outage Count\")\n",
    "ax.set_title(\"Predicted Outage Causes by Month\")\n",
    "ax.legend(title=\"Cause\", bbox_to_anchor=(1.05, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Built and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# For reproducible results, set random seeds at the top of your notebook\n",
    "np.random.seed(42)\n",
    "\n",
    "# Save the trained model for later use\n",
    "import joblib\n",
    "joblib.dump(model, \"outage_cause_model.pkl\")\n",
    "\n",
    "# Load it back\n",
    "model = joblib.load(\"outage_cause_model.pkl\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}