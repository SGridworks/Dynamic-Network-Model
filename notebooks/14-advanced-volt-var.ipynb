{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "name": "14-advanced-volt-var.ipynb"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network for Multi-Device Volt-VAR Control\n",
    "\n",
    "From the [Sisyphean Gridworks ML Playground](https://sgridworks.com/ml-playground/guides/14-advanced-volt-var.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Clone the repository and install dependencies. Run this cell first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone https://github.com/SGridworks/Dynamic-Network-Model.git 2>/dev/null || echo 'Already cloned'\n",
    "%cd Dynamic-Network-Model\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm pyarrow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Why Q-Tables Cannot Scale\n",
    "\n",
    "In Guide 06, the Q-learning agent used a small table indexed by (voltage_bucket, cap_state). The voltage reading was discretized into 5 buckets, and there were 2 capacitor states, giving a total of 10 entries in the Q-table. This worked because the problem was simple: one continuous reading, one binary control."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Guide 06 Q-table: 5 voltage buckets x 2 cap states x 2 actions = 20 entries\n",
    "q_table_guide06 = np.zeros((5, 2, 2))\n",
    "print(f\"Guide 06 Q-table size: {q_table_guide06.size} entries\")\n",
    "\n",
    "# Now consider the multi-device problem:\n",
    "#   - 15 monitored buses, each with voltage discretized into 10 buckets\n",
    "#   - 3 capacitor banks, each ON/OFF (2^3 = 8 combinations)\n",
    "#   - 2 regulators, each with 33 tap positions (33^2 = 1,089 combinations)\n",
    "#   - 4 smart inverters, each with 5 VAR setpoints (5^4 = 625 combinations)\n",
    "n_voltage_states = 10 ** 15        # 10 buckets per bus, 15 buses\n",
    "n_device_states = 8 * 1089 * 625  # all device combinations\n",
    "n_actions = 8 * 1089 * 625        # can set any device combination\n",
    "\n",
    "print(f\"\\nMulti-device Q-table would need:\")\n",
    "print(f\"  State space:  {n_voltage_states * n_device_states:.2e}\")\n",
    "print(f\"  Action space: {n_actions:,}\")\n",
    "print(f\"  Q-table entries: {n_voltage_states * n_device_states * n_actions:.2e}\")\n",
    "print(f\"  That's impossibly large. We need function approximation.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Multi-Device VVO Environment\n",
    "\n",
    "We expand the single-device environment from Guide 06 into a multi-device environment. The state vector now includes continuous voltage readings at all monitored buses, capacitor bank statuses, regulator tap positions, and smart inverter VAR setpoints. Actions control all devices simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from demo_data.load_demo_data import load_load_profiles, load_solar_profiles, load_network_nodes\n",
    "\n",
    "# Load SP&L datasets via the data-loader API\n",
    "load_profiles = load_load_profiles()        # 15-min load + voltage_pu per feeder\n",
    "solar_profiles = load_solar_profiles()      # hourly generation curves\n",
    "network_nodes = load_network_nodes()        # node locations and equipment classes\n",
    "\n",
    "# Filter to a single feeder for the RL environment\n",
    "feeder_id = load_profiles[\"feeder_id\"].iloc[0]\n",
    "feeder_load = load_profiles[load_profiles[\"feeder_id\"] == feeder_id].reset_index(drop=True)\n",
    "feeder_pv = solar_profiles.reset_index(drop=True)\n",
    "\n",
    "# Device configuration for feeder FDR-0003\n",
    "N_BUSES = 15           # monitored voltage measurement points\n",
    "N_CAPS = 3             # capacitor banks (each ON/OFF)\n",
    "N_REGS = 2             # regulators (tap positions -16 to +16, discretized to 5 levels)\n",
    "N_INVERTERS = 4        # smart inverters (5 VAR setpoints each)\n",
    "REG_TAP_LEVELS = 5     # discretized tap positions: [-8, -4, 0, +4, +8]\n",
    "INV_VAR_LEVELS = 5     # setpoints: [-100%, -50%, 0%, +50%, +100%] of rated kVAR\n",
    "\n",
    "# State: voltages + device states (continuous vector)\n",
    "STATE_DIM = N_BUSES + N_CAPS + N_REGS + N_INVERTERS  # 15 + 3 + 2 + 4 = 24\n",
    "\n",
    "# Actions: encode as discrete combinations\n",
    "# Each action selects: cap_combo (2^3=8) x reg_combo (5^2=25) x inv_combo (5^4=625)\n",
    "# Full action space = 125,000 -- too many for DQN output layer\n",
    "# Instead, use 27 \"adjustment\" actions (see below)\n",
    "N_ACTIONS = 27\n",
    "\n",
    "print(f\"State dimension:  {STATE_DIM}\")\n",
    "print(f\"Action space:     {N_ACTIONS} discrete adjustment actions\")\n",
    "print(f\"Monitored buses:  {N_BUSES}\")\n",
    "print(f\"Control devices:  {N_CAPS} caps + {N_REGS} regs + {N_INVERTERS} inverters\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the DQN Architecture in PyTorch\n",
    "\n",
    "The core idea of DQN: replace the Q-table with a neural network. The network takes the state vector (24 dimensions) as input and outputs a Q-value for each of the 27 possible actions. The action with the highest predicted Q-value is the one the agent selects."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class MultiDeviceVVOEnv:\n",
    "    \"\"\"Multi-device Volt-VAR environment for DQN training.\n",
    "\n",
    "    State vector (24 dims):\n",
    "        [0:15]  - voltage p.u. at each monitored bus\n",
    "        [15:18] - capacitor bank status (0=OFF, 1=ON)\n",
    "        [18:20] - regulator tap position (normalized to [-1, 1])\n",
    "        [20:24] - smart inverter VAR setpoint (normalized to [-1, 1])\n",
    "\n",
    "    Actions (27 discrete):\n",
    "        Combinations of {raise, hold, lower} for three device groups:\n",
    "        caps (3 options) x regs (3 options) x inverters (3 options) = 27\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, load_data, pv_data, n_hours=24):\n",
    "        self.load_data = load_data\n",
    "        self.pv_data = pv_data\n",
    "        self.n_hours = n_hours\n",
    "\n",
    "        # Device state arrays\n",
    "        self.cap_states = np.zeros(N_CAPS)          # 0 or 1\n",
    "        self.reg_taps = np.zeros(N_REGS)             # normalized [-1, 1]\n",
    "        self.inv_setpoints = np.zeros(N_INVERTERS)   # normalized [-1, 1]\n",
    "        self.hour = 0\n",
    "        self.day_offset = 0\n",
    "\n",
    "        # Decode 27 actions into adjustment commands\n",
    "        self.action_map = []\n",
    "        for cap_adj in [-1, 0, 1]:\n",
    "            for reg_adj in [-1, 0, 1]:\n",
    "                for inv_adj in [-1, 0, 1]:\n",
    "                    self.action_map.append((cap_adj, reg_adj, inv_adj))\n",
    "\n",
    "    def reset(self, day_offset=None):\n",
    "        \"\"\"Reset to beginning of a 24-hour episode.\"\"\"\n",
    "        self.cap_states = np.zeros(N_CAPS)\n",
    "        self.reg_taps = np.zeros(N_REGS)\n",
    "        self.inv_setpoints = np.zeros(N_INVERTERS)\n",
    "        self.hour = 0\n",
    "        if day_offset is not None:\n",
    "            self.day_offset = day_offset\n",
    "        else:\n",
    "            self.day_offset = np.random.randint(0, len(self.load_data) - self.n_hours)\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_voltages(self):\n",
    "        \"\"\"Read voltage from load_profiles and simulate device effects.\"\"\"\n",
    "        idx = self.day_offset + self.hour\n",
    "        # Read the actual voltage_pu from SP&L load profiles\n",
    "        base_voltage = self.load_data.iloc[idx][\"voltage_pu\"]\n",
    "        # Spread across monitored buses with small spatial gradient\n",
    "        base_v = base_voltage - 0.005 * np.linspace(0, 1, N_BUSES)\n",
    "\n",
    "        # Simulate device effects on voltage\n",
    "        # Capacitor: each ON cap boosts voltage by ~0.02 p.u.\n",
    "        cap_boost = np.sum(self.cap_states) * 0.02\n",
    "        reg_boost = np.mean(self.reg_taps) * 0.015\n",
    "        inv_boost = np.mean(self.inv_setpoints) * 0.006\n",
    "        voltages = base_v + cap_boost + reg_boost + inv_boost\n",
    "        # Add small random noise to simulate measurement uncertainty\n",
    "        voltages += np.random.normal(0, 0.002, N_BUSES)\n",
    "        return np.clip(voltages, 0.85, 1.15)\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Build the 24-dim state vector.\"\"\"\n",
    "        voltages = self._get_voltages()\n",
    "        return np.concatenate([\n",
    "            voltages,\n",
    "            self.cap_states,\n",
    "            self.reg_taps,\n",
    "            self.inv_setpoints\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "    def _apply_action(self, action_idx):\n",
    "        \"\"\"Apply adjustment action to all device groups.\"\"\"\n",
    "        cap_adj, reg_adj, inv_adj = self.action_map[action_idx]\n",
    "        prev_caps = self.cap_states.copy()\n",
    "        prev_taps = self.reg_taps.copy()\n",
    "\n",
    "        # Toggle capacitors: adj=+1 turns next OFF cap ON, adj=-1 turns last ON cap OFF\n",
    "        if cap_adj == 1:\n",
    "            off_caps = np.where(self.cap_states == 0)[0]\n",
    "            if len(off_caps) > 0:\n",
    "                self.cap_states[off_caps[0]] = 1\n",
    "        elif cap_adj == -1:\n",
    "            on_caps = np.where(self.cap_states == 1)[0]\n",
    "            if len(on_caps) > 0:\n",
    "                self.cap_states[on_caps[-1]] = 0\n",
    "\n",
    "        # Adjust regulator taps\n",
    "        self.reg_taps = np.clip(self.reg_taps + reg_adj * 0.25, -1.0, 1.0)\n",
    "\n",
    "        # Adjust inverter VAR setpoints\n",
    "        self.inv_setpoints = np.clip(self.inv_setpoints + inv_adj * 0.25, -1.0, 1.0)\n",
    "\n",
    "        # Count switching operations for penalty\n",
    "        n_switches = int(np.sum(self.cap_states != prev_caps))\n",
    "        n_switches += int(np.sum(self.reg_taps != prev_taps))\n",
    "        return n_switches\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        \"\"\"Execute one timestep: apply action, advance, return (state, reward, done, info).\"\"\"\n",
    "        n_switches = self._apply_action(action_idx)\n",
    "        self.hour += 1\n",
    "        done = self.hour >= self.n_hours\n",
    "\n",
    "        state = self._get_state()\n",
    "        voltages = state[:N_BUSES]\n",
    "\n",
    "        # Compute reward (defined in Step 6)\n",
    "        reward, info = self._compute_reward(voltages, n_switches)\n",
    "        info[\"voltages\"] = voltages\n",
    "        info[\"hour\"] = self.hour\n",
    "\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def _compute_reward(self, voltages, n_switches):\n",
    "        \"\"\"Multi-objective reward (detailed in Step 6).\"\"\"\n",
    "        # Voltage violation penalty\n",
    "        violations = np.sum((voltages 0.95) | (voltages > 1.05))\n",
    "        v_penalty = -5.0 * violations\n",
    "\n",
    "        # Deviation from 1.0 p.u. (proxy for losses)\n",
    "        deviation = np.mean((voltages - 1.0) ** 2)\n",
    "        loss_penalty = -10.0 * deviation\n",
    "\n",
    "        # Switching penalty\n",
    "        switch_penalty = -0.5 * n_switches\n",
    "\n",
    "        # Bonus for all voltages in range\n",
    "        all_ok = 2.0 if violations == 0 else 0.0\n",
    "\n",
    "        reward = v_penalty + loss_penalty + switch_penalty + all_ok\n",
    "\n",
    "        info = {\n",
    "            \"violations\": violations,\n",
    "            \"mean_deviation\": deviation,\n",
    "            \"n_switches\": n_switches,\n",
    "            \"reward_breakdown\": {\n",
    "                \"voltage\": v_penalty,\n",
    "                \"loss\": loss_penalty,\n",
    "                \"switching\": switch_penalty,\n",
    "                \"bonus\": all_ok\n",
    "            }\n",
    "        }\n",
    "        return reward, info\n",
    "\n",
    "# Test the environment\n",
    "env = MultiDeviceVVOEnv(feeder_load, feeder_pv, n_hours=24)\n",
    "state = env.reset(day_offset=0)\n",
    "print(f\"Initial state shape: {state.shape}\")\n",
    "print(f\"Bus voltages: {state[:5].round(4)} ... (first 5 of {N_BUSES})\")\n",
    "print(f\"Cap states:   {state[15:18]}\")\n",
    "print(f\"Reg taps:     {state[18:20]}\")\n",
    "print(f\"Inv setpoints:{state[20:24]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Experience Replay Buffer\n",
    "\n",
    "In Q-learning (Guide 06), we updated the Q-table immediately after every step. This creates a problem for neural networks: consecutive experiences are highly correlated (hour 3 looks a lot like hour 4), which destabilizes gradient descent. Experience replay stores transitions in a buffer and trains on random mini-batches, breaking temporal correlation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network: maps state vector to Q-values for each action.\n",
    "\n",
    "    Architecture:\n",
    "        Input  (24) -> Dense(128) -> ReLU -> Dense(128) -> ReLU -> Dense(64) -> ReLU -> Output(27)\n",
    "\n",
    "    The network learns: Q(state, action) \u2248 expected cumulative reward\n",
    "    for taking 'action' in 'state' and following the optimal policy after.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: state tensor -> Q-values for all actions.\"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "# Initialize the Q-network\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "q_network = DQNetwork(STATE_DIM, N_ACTIONS).to(device)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Network architecture:\")\n",
    "print(q_network)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in q_network.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "q_values = q_network(test_state)\n",
    "print(f\"\\nTest Q-values shape: {q_values.shape}\")\n",
    "print(f\"Best action: {q_values.argmax(dim=1).item()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Target Network\n",
    "\n",
    "DQN uses two copies of the Q-network: the online network (updated every step via gradient descent) and the target network (a frozen copy updated only periodically). The target network provides stable Q-value targets during training, preventing a feedback loop where the network chases its own rapidly changing predictions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\n",
    "\n",
    "    Each experience is (state, action, reward, next_state, done).\n",
    "    Training samples random mini-batches to break temporal correlation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity=50000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)).to(device),\n",
    "            torch.LongTensor(actions).to(device),\n",
    "            torch.FloatTensor(rewards).to(device),\n",
    "            torch.FloatTensor(np.array(next_states)).to(device),\n",
    "            torch.FloatTensor(dones).to(device),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize buffer\n",
    "replay_buffer = ReplayBuffer(capacity=50000)\n",
    "print(f\"Replay buffer initialized (capacity: 50,000 transitions)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Multi-Objective Reward Function\n",
    "\n",
    "The reward function is already implemented in the environment (Step 2), but it deserves a detailed explanation. VVO has three competing objectives that the reward must balance:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import copy\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with experience replay and target network.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, n_actions, lr=1e-3, gamma=0.99,\n",
    "                 epsilon_start=1.0, epsilon_end=0.02, epsilon_decay=0.995,\n",
    "                 target_update_freq=100, batch_size=64):\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.train_step = 0\n",
    "\n",
    "        # Online network: updated every training step\n",
    "        self.q_network = DQNetwork(state_dim, n_actions).to(device)\n",
    "\n",
    "        # Target network: frozen copy, updated periodically\n",
    "        self.target_network = copy.deepcopy(self.q_network)\n",
    "        self.target_network.eval()  # never in training mode\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer(capacity=50000)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.random() return np.random.randint(self.n_actions)\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = self.q_network(state_t)\n",
    "            return q_values.argmax(dim=1).item()\n",
    "\n",
    "    def train_on_batch(self):\n",
    "        \"\"\"Sample a batch from replay buffer and update the Q-network.\"\"\"\n",
    "        if len(self.replay_buffer) return None\n",
    "\n",
    "        states, actions, rewards, next_states, dones = \\\n",
    "            self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # Current Q-values: Q(s, a) from online network\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Target Q-values: r + gamma * max_a' Q_target(s', a')\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(dim=1)[0]\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "\n",
    "        # Compute loss and update\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.train_step += 1\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy online network weights to target network.\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduce exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# Initialize the agent\n",
    "agent = DQNAgent(\n",
    "    state_dim=STATE_DIM,\n",
    "    n_actions=N_ACTIONS,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.02,\n",
    "    epsilon_decay=0.995,\n",
    "    target_update_freq=100,\n",
    "    batch_size=64,\n",
    ")\n",
    "print(\"DQN Agent initialized.\")\n",
    "print(f\"  Online network params:  {sum(p.numel() for p in agent.q_network.parameters()):,}\")\n",
    "print(f\"  Target network params:  {sum(p.numel() for p in agent.target_network.parameters()):,}\")\n",
    "print(f\"  Target update every:    {agent.target_update_freq} episodes\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DQN Agent\n",
    "\n",
    "Each training episode simulates a full 24-hour day. The agent starts with random exploration (high epsilon) and gradually shifts to exploiting its learned policy. We train for 500 episodes, which represents 500 simulated days of VVO operation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Reward function breakdown (from MultiDeviceVVOEnv._compute_reward):\n",
    "#\n",
    "# 1. VOLTAGE VIOLATION PENALTY: -5.0 per bus outside [0.95, 1.05] p.u.\n",
    "#    This is the primary safety constraint. ANSI C84.1 Range A requires\n",
    "#    service voltage within +/- 5% of nominal. Violations can damage\n",
    "#    customer equipment and trigger regulatory penalties.\n",
    "#\n",
    "# 2. LOSS MINIMIZATION: -10.0 * mean((V - 1.0)^2)\n",
    "#    Voltage deviation from nominal is a proxy for reactive power losses.\n",
    "#    Keeping voltage close to 1.0 p.u. across the feeder minimizes I^2R\n",
    "#    losses and improves efficiency (conservation voltage reduction).\n",
    "#\n",
    "# 3. SWITCHING PENALTY: -0.5 per switching operation\n",
    "#    Excessive switching wears out mechanical equipment (cap bank switches,\n",
    "#    regulator tap changers). Utilities limit operations to ~6 per day.\n",
    "#    This penalty encourages the agent to find stable setpoints.\n",
    "#\n",
    "# 4. COMPLIANCE BONUS: +2.0 when ALL buses are within ANSI limits\n",
    "#    Rewards the agent for achieving the primary objective.\n",
    "\n",
    "# Demonstrate the reward components on a sample step\n",
    "env = MultiDeviceVVOEnv(feeder_load, feeder_pv, n_hours=24)\n",
    "state = env.reset(day_offset=0)\n",
    "\n",
    "# Take a \"do nothing\" action (hold all devices)\n",
    "hold_action = 13  # (0, 0, 0) = hold caps, hold regs, hold inverters\n",
    "next_state, reward, done, info = env.step(hold_action)\n",
    "\n",
    "print(\"Reward breakdown for 'hold all' action:\")\n",
    "for component, value in info[\"reward_breakdown\"].items():\n",
    "    print(f\"  {component:<12s}: {value:+.3f}\")\n",
    "print(f\"  {'TOTAL':<12s}: {reward:+.3f}\")\n",
    "print(f\"\\nVoltage violations: {info['violations']} of {N_BUSES} buses\")\n",
    "print(f\"Mean V deviation:   {info['mean_deviation']:.6f}\")\n",
    "print(f\"Switching ops:      {info['n_switches']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate: DQN vs Rule-Based vs Q-Learning\n",
    "\n",
    "Run all three controllers on the same 30-day evaluation window and compare three key operational metrics: voltage violation minutes, total deviation (proxy for losses), and number of switching operations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training configuration\n",
    "N_EPISODES = 500\n",
    "LOG_INTERVAL = 50\n",
    "\n",
    "# Tracking metrics\n",
    "episode_rewards = []\n",
    "episode_violations = []\n",
    "episode_losses = []\n",
    "\n",
    "env = MultiDeviceVVOEnv(feeder_load, feeder_pv, n_hours=24)\n",
    "\n",
    "for ep in range(N_EPISODES):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    total_violations = 0\n",
    "    ep_losses = []\n",
    "\n",
    "    for t in range(24):\n",
    "        # Select action with epsilon-greedy policy\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # Execute in environment\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Store transition in replay buffer\n",
    "        agent.replay_buffer.push(\n",
    "            state, action, reward, next_state, float(done)\n",
    "        )\n",
    "\n",
    "        # Train on a random batch\n",
    "        loss = agent.train_on_batch()\n",
    "        if loss is not None:\n",
    "            ep_losses.append(loss)\n",
    "\n",
    "        total_reward += reward\n",
    "        total_violations += info[\"violations\"]\n",
    "        state = next_state\n",
    "\n",
    "    # Update target network periodically\n",
    "    if (ep + 1) % agent.target_update_freq == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    # Decay exploration\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "    # Track metrics\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_violations.append(total_violations)\n",
    "    episode_losses.append(np.mean(ep_losses) if ep_losses else 0)\n",
    "\n",
    "    if (ep + 1) % LOG_INTERVAL == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-LOG_INTERVAL:])\n",
    "        avg_viols = np.mean(episode_violations[-LOG_INTERVAL:])\n",
    "        print(f\"Episode {ep+1:>4}/{N_EPISODES}  \"\n",
    "              f\"Avg Reward: {avg_reward:>7.1f}  \"\n",
    "              f\"Avg Violations: {avg_viols:>5.1f}  \"\n",
    "              f\"Epsilon: {agent.epsilon:.3f}  \"\n",
    "              f\"Loss: {episode_losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete. Buffer size: {len(agent.replay_buffer):,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generalization on High-Variability Days\n",
    "\n",
    "A critical question for any ML-based controller: does it work on conditions it has never seen? Cloud transients cause rapid swings in solar generation, creating voltage fluctuations that stress VVO controllers. We evaluate the trained DQN on days with the highest PV variability in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Reward curve\n",
    "ax = axes[0]\n",
    "ax.plot(episode_rewards, alpha=0.3, color=\"#5FCCDB\")\n",
    "ax.plot(pd.Series(episode_rewards).rolling(20).mean(),\n",
    "       color=\"#1C4855\", linewidth=2, label=\"20-episode avg\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Total Episode Reward\")\n",
    "ax.set_title(\"DQN Training: Reward\")\n",
    "ax.legend()\n",
    "\n",
    "# Violation curve\n",
    "ax = axes[1]\n",
    "ax.plot(episode_violations, alpha=0.3, color=\"#fc8181\")\n",
    "ax.plot(pd.Series(episode_violations).rolling(20).mean(),\n",
    "       color=\"#c53030\", linewidth=2, label=\"20-episode avg\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Total Voltage Violations\")\n",
    "ax.set_title(\"DQN Training: Violations\")\n",
    "ax.legend()\n",
    "\n",
    "# Loss curve\n",
    "ax = axes[2]\n",
    "ax.plot(episode_losses, alpha=0.3, color=\"#fbd38d\")\n",
    "ax.plot(pd.Series(episode_losses).rolling(20).mean(),\n",
    "       color=\"#d69e2e\", linewidth=2, label=\"20-episode avg\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Mean MSE Loss\")\n",
    "ax.set_title(\"DQN Training: Loss\")\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(\"DQN Training Progress for Multi-Device VVO\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence and Hyperparameter Justification\n",
    "\n",
    "Save the trained DQN weights so you can deploy the agent or resume training later without retraining from scratch."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_dqn(agent, env, n_days=30):\n",
    "    \"\"\"Run trained DQN agent for n_days and collect metrics.\"\"\"\n",
    "    all_violations = 0\n",
    "    all_deviation = 0.0\n",
    "    all_switches = 0\n",
    "    all_rewards = 0.0\n",
    "    hourly_voltages = []\n",
    "\n",
    "    for day in range(n_days):\n",
    "        state = env.reset(day_offset=day * 24)\n",
    "        for t in range(24):\n",
    "            with torch.no_grad():\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                action = agent.q_network(state_t).argmax(dim=1).item()\n",
    "            state, reward, done, info = env.step(action)\n",
    "            all_violations += info[\"violations\"]\n",
    "            all_deviation += info[\"mean_deviation\"]\n",
    "            all_switches += info[\"n_switches\"]\n",
    "            all_rewards += reward\n",
    "            hourly_voltages.append(info[\"voltages\"].mean())\n",
    "\n",
    "    return {\n",
    "        \"violation_minutes\": all_violations * 60,  # each hour step = 60 min\n",
    "        \"total_deviation\": all_deviation,\n",
    "        \"switching_ops\": all_switches,\n",
    "        \"total_reward\": all_rewards,\n",
    "        \"hourly_voltages\": hourly_voltages,\n",
    "    }\n",
    "\n",
    "def evaluate_rule_based(env, n_days=30):\n",
    "    \"\"\"Run simple rule-based controller from Guide 06.\"\"\"\n",
    "    all_violations = 0\n",
    "    all_deviation = 0.0\n",
    "    all_switches = 0\n",
    "    all_rewards = 0.0\n",
    "    hourly_voltages = []\n",
    "\n",
    "    for day in range(n_days):\n",
    "        state = env.reset(day_offset=day * 24)\n",
    "        for t in range(24):\n",
    "            mean_v = state[:N_BUSES].mean()\n",
    "            # Rule: raise if low, lower if high, hold otherwise\n",
    "            if mean_v 0.97:\n",
    "                action = 26  # raise all: (+1, +1, +1)\n",
    "            elif mean_v > 1.03:\n",
    "                action = 0   # lower all: (-1, -1, -1)\n",
    "            else:\n",
    "                action = 13  # hold all: (0, 0, 0)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            all_violations += info[\"violations\"]\n",
    "            all_deviation += info[\"mean_deviation\"]\n",
    "            all_switches += info[\"n_switches\"]\n",
    "            all_rewards += reward\n",
    "            hourly_voltages.append(info[\"voltages\"].mean())\n",
    "\n",
    "    return {\n",
    "        \"violation_minutes\": all_violations * 60,\n",
    "        \"total_deviation\": all_deviation,\n",
    "        \"switching_ops\": all_switches,\n",
    "        \"total_reward\": all_rewards,\n",
    "        \"hourly_voltages\": hourly_voltages,\n",
    "    }\n",
    "\n",
    "# Run evaluations\n",
    "env_eval = MultiDeviceVVOEnv(feeder_load, feeder_pv, n_hours=24)\n",
    "\n",
    "dqn_metrics = evaluate_dqn(agent, env_eval, n_days=30)\n",
    "rule_metrics = evaluate_rule_based(env_eval, n_days=30)\n",
    "\n",
    "# Display comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"Violation Minutes\", \"Total Deviation (loss proxy)\",\n",
    "              \"Switching Operations\", \"Total Reward\"],\n",
    "    \"Rule-Based\": [\n",
    "        f\"{rule_metrics['violation_minutes']:,.0f}\",\n",
    "        f\"{rule_metrics['total_deviation']:.3f}\",\n",
    "        f\"{rule_metrics['switching_ops']}\",\n",
    "        f\"{rule_metrics['total_reward']:.1f}\",\n",
    "    ],\n",
    "    \"Q-Learning (Guide 06)\": [\n",
    "        \"~840\", \"~4.2\", \"~95\", \"~620\"  # approximate values from Guide 06 (exact numbers depend on Q-learning training run)\n",
    "    ],\n",
    "    \"DQN (This Guide)\": [\n",
    "        f\"{dqn_metrics['violation_minutes']:,.0f}\",\n",
    "        f\"{dqn_metrics['total_deviation']:.3f}\",\n",
    "        f\"{dqn_metrics['switching_ops']}\",\n",
    "        f\"{dqn_metrics['total_reward']:.1f}\",\n",
    "    ],\n",
    "})\n",
    "print(\"30-Day Evaluation Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Built and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot a sample day comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "hours = range(24)\n",
    "\n",
    "# Rule-based\n",
    "ax = axes[0]\n",
    "ax.plot(hours, rule_metrics[\"hourly_voltages\"][:24], \"o-\", color=\"#2D6A7A\",\n",
    "       markersize=5, label=\"Mean bus voltage\")\n",
    "ax.axhspan(0.95, 1.05, alpha=0.1, color=\"green\")\n",
    "ax.axhline(1.0, color=\"gray\", linestyle=\":\", alpha=0.5)\n",
    "ax.set_title(\"Rule-Based Controller\")\n",
    "ax.set_xlabel(\"Hour of Day\")\n",
    "ax.set_ylabel(\"Mean Voltage (p.u.)\")\n",
    "ax.legend()\n",
    "\n",
    "# DQN\n",
    "ax = axes[1]\n",
    "ax.plot(hours, dqn_metrics[\"hourly_voltages\"][:24], \"o-\", color=\"#5FCCDB\",\n",
    "       markersize=5, label=\"Mean bus voltage\")\n",
    "ax.axhspan(0.95, 1.05, alpha=0.1, color=\"green\")\n",
    "ax.axhline(1.0, color=\"gray\", linestyle=\":\", alpha=0.5)\n",
    "ax.set_title(\"DQN Controller\")\n",
    "ax.set_xlabel(\"Hour of Day\")\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(\"VVO Controller Comparison: Day 1\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Find the most variable PV days (cloud transients)\n",
    "feeder_pv[\"date\"] = pd.to_datetime(feeder_pv[\"timestamp\"]).dt.date\n",
    "daily_pv_std = feeder_pv.groupby(\"date\")[\"clear_sky_factor\"].std()\n",
    "high_var_days = daily_pv_std.nlargest(10)\n",
    "\n",
    "print(\"Top 10 highest PV variability days (cloud transients):\")\n",
    "print(high_var_days)\n",
    "\n",
    "# Evaluate DQN on these challenging days\n",
    "hard_dqn_violations = []\n",
    "hard_rule_violations = []\n",
    "\n",
    "for day_date in high_var_days.index:\n",
    "    # Find the day offset in the time series\n",
    "    day_mask = feeder_pv[\"date\"] == day_date\n",
    "    if day_mask.sum() 24:\n",
    "        continue\n",
    "    day_start = feeder_pv[day_mask].index[0]\n",
    "\n",
    "    # DQN evaluation\n",
    "    state = env_eval.reset(day_offset=day_start)\n",
    "    day_viols_dqn = 0\n",
    "    for t in range(24):\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action = agent.q_network(state_t).argmax(dim=1).item()\n",
    "        state, _, _, info = env_eval.step(action)\n",
    "        day_viols_dqn += info[\"violations\"]\n",
    "    hard_dqn_violations.append(day_viols_dqn)\n",
    "\n",
    "    # Rule-based evaluation\n",
    "    state = env_eval.reset(day_offset=day_start)\n",
    "    day_viols_rule = 0\n",
    "    for t in range(24):\n",
    "        mean_v = state[:N_BUSES].mean()\n",
    "        if mean_v 0.97:\n",
    "            action = 26\n",
    "        elif mean_v > 1.03:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 13\n",
    "        state, _, _, info = env_eval.step(action)\n",
    "        day_viols_rule += info[\"violations\"]\n",
    "    hard_rule_violations.append(day_viols_rule)\n",
    "\n",
    "# Compare on hard days\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(hard_dqn_violations))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, hard_rule_violations, width,\n",
    "     label=\"Rule-Based\", color=\"#2D6A7A\")\n",
    "ax.bar(x + width/2, hard_dqn_violations, width,\n",
    "     label=\"DQN\", color=\"#5FCCDB\")\n",
    "ax.set_xlabel(\"High-Variability Day (ranked by PV std)\")\n",
    "ax.set_ylabel(\"Voltage Violations (bus-hours)\")\n",
    "ax.set_title(\"Generalization Test: DQN vs Rule-Based on Unseen Cloud Transient Days\")\n",
    "ax.legend()\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"Day {i+1}\" for i in x])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nHigh-variability day results:\")\n",
    "print(f\"  Rule-based avg violations: {np.mean(hard_rule_violations):.1f} bus-hours/day\")\n",
    "print(f\"  DQN avg violations:        {np.mean(hard_dqn_violations):.1f} bus-hours/day\")\n",
    "print(f\"  DQN reduction:             {(1 - np.mean(hard_dqn_violations)/np.mean(hard_rule_violations))*100:.0f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save trained DQN weights\n",
    "torch.save(agent.q_network.state_dict(), \"vvo_dqn.pt\")\n",
    "\n",
    "# Load: agent.q_network.load_state_dict(torch.load(\"vvo_dqn.pt\"))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}